{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "366971587it [08:13, 743924.55it/s]\n",
      "100%|██████████| 367008/367008 [00:33<00:00, 10878.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "qid2line = defaultdict(list)\n",
    "qrel = defaultdict(list)\n",
    "with open(\"collections/msmarco-doc/msmarco-doctrain-qrels.tsv\") as f:\n",
    "    for line in f:\n",
    "        topicid, _, docid, rel = line.strip().split(' ')\n",
    "        assert rel == \"1\", line.split(' ')\n",
    "        qrel[topicid].append(docid)\n",
    "\n",
    "with open('runs/msmarco-doc/run.msmarco-doc.bm25-tuned.topics.msmarco-doc.train.txt') as f:\n",
    "    for line in tqdm(f):\n",
    "        topicid, _, _, _, _, _ = line.strip().split()\n",
    "        if topicid in qrel:\n",
    "            qid2line[topicid].append(line.strip())\n",
    "\n",
    "with open('runs/msmarco-doc/run.train.small.tsv','w') as f:\n",
    "    for topicid,lines in tqdm(qid2line.items()):\n",
    "        f.write('\\n'.join(lines)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271184/271184 [00:11<00:00, 24368.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import gzip\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The query string for each topicid is querystring[topicid]\n",
    "querystring = {}\n",
    "with gzip.open(\"collections/msmarco-doc/msmarco-doctrain-queries.tsv.gz\", 'rt', encoding='utf8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for [topicid, querystring_of_topicid] in tsvreader:\n",
    "        querystring[topicid] = querystring_of_topicid\n",
    "\n",
    "# In the corpus tsv, each docid occurs at offset docoffset[docid]\n",
    "docoffset = {}\n",
    "with gzip.open(\"collections/msmarco-doc/msmarco-docs-lookup.tsv.gz\", 'rt', encoding='utf8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for [docid, _, offset] in tsvreader:\n",
    "        docoffset[docid] = int(offset)\n",
    "\n",
    "# For each topicid, the list of positive docids is qrel[topicid]\n",
    "qrel = {}\n",
    "with gzip.open(\"collections/msmarco-doc/msmarco-doctrain-qrels.tsv.gz\", 'rt', encoding='utf8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\" \")\n",
    "    for [topicid, _, docid, rel] in tsvreader:\n",
    "        assert rel == \"1\"\n",
    "        if topicid in qrel:\n",
    "            qrel[topicid].append(docid)\n",
    "        else:\n",
    "            qrel[topicid] = [docid]\n",
    "\n",
    "def generate_triples(outfile):\n",
    "    \"\"\"Generates triples comprising:\n",
    "    - Query: The current topicid and query string\n",
    "    - Pos: One of the positively-judged documents for this query\n",
    "    - Rnd: Any of the top-100 documents for this query other than Pos\n",
    "    Since we have the URL, title and body of each document, this gives us ten columns in total:\n",
    "    topicid, query, posdocid, posurl, postitle, posbody, rnddocid, rndurl, rndtitle, rndbody\n",
    "    outfile: The filename where the triples are written\n",
    "    triples_to_generate: How many triples to generate\n",
    "    \"\"\"\n",
    "\n",
    "    qid2pos = defaultdict(list)\n",
    "    qid2neg = defaultdict(list)\n",
    "\n",
    "    with gzip.open(\"collections/msmarco-doc/msmarco-doctrain-top100.gz\", 'rt', encoding='utf8') as top100f,\\\n",
    "    open(outfile, 'w', encoding=\"utf8\") as out:\n",
    "        for line in top100f:\n",
    "            [topicid, _, unjudged_docid, rank, _, _] = line.split()\n",
    "\n",
    "            assert topicid in querystring\n",
    "            assert topicid in qrel\n",
    "            assert unjudged_docid in docoffset\n",
    "\n",
    "            if unjudged_docid in qrel[topicid]:\n",
    "                qid2pos[topicid].append(unjudged_docid)\n",
    "            else:\n",
    "                qid2neg[topicid].append(unjudged_docid)\n",
    "    \n",
    "        for topicid, pos_list in tqdm(qid2pos.items()):\n",
    "            for negative_docid in qid2neg[topicid]:\n",
    "                for positive_docid in pos_list:\n",
    "                    out.write(topicid + \"\\t\" +\n",
    "                              positive_docid + \"\\t\" +\n",
    "                              negative_docid + \"\\n\")\n",
    "\n",
    "\n",
    "generate_triples(\"triples.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3213835it [05:28, 9784.11it/s] \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "f_corpus = gzip.open('collections/msmarco-doc/msmarco-docs.tsv.gz', mode='rt')\n",
    "total_doc = 0\n",
    "f_jsonl = open(f'collections/msmarco-doc/collection_jsonl/msmarco-doc_0.jsonl','w')\n",
    "for line in tqdm(f_corpus):\n",
    "    if total_doc % 100000 == 0 and total_doc != 0:\n",
    "        f_jsonl.close()\n",
    "        f_jsonl = open(f'collections/msmarco-doc/collection_jsonl/msmarco-doc_{total_doc//100000}.jsonl','w')\n",
    "    f_jsonl.write(line)\n",
    "    total_doc += 1\n",
    "f_corpus.close()\n",
    "f_jsonl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T16:15:32.502734Z",
     "start_time": "2020-12-08T15:42:06.116564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from glob import glob\n",
    "\n",
    "def process(corpus_fn):\n",
    "    max_length = 10\n",
    "    stride = 5\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    def create_segments(doc_text, nlp, max_length, stride):\n",
    "        doc_text = doc_text.strip()\n",
    "    #     if len(doc_text) > 100000:\n",
    "    #         print(len(doc_text))\n",
    "        doc = nlp(doc_text[:10000])\n",
    "        sentences = [sent.string.strip() for sent in doc.sents]\n",
    "        segments = []\n",
    "\n",
    "        for i in range(0, len(sentences), stride):\n",
    "            segment = \" \".join(sentences[i:i+max_length])\n",
    "            segments.append(segment)\n",
    "            if i + max_length >= len(sentences):\n",
    "                break\n",
    "        return segments\n",
    "    \n",
    "    total_seg = 0\n",
    "    new_name = corpus_fn.replace('msmarco-doc_','msmarco-seg_')\n",
    "    assert new_name != corpus_fn\n",
    "    with open(corpus_fn) as f_corpus, open(new_name,'w') as f_jsonl:\n",
    "        for line in tqdm(f_corpus):\n",
    "            f_doc_id, doc_url, doc_title, doc_text = line.split('\\t')\n",
    "            segments = create_segments(doc_text, nlp, max_length, stride)\n",
    "            for seg_id, segment in enumerate(segments):\n",
    "                doc_seg = f'{f_doc_id}#{seg_id}'\n",
    "                expanded_text = f'{doc_url} {doc_title} {segment}'\n",
    "                output_dict = {'id': doc_seg, 'contents': expanded_text}\n",
    "                f_jsonl.write(json.dumps(output_dict) + '\\n')\n",
    "                total_seg += 1\n",
    "    \n",
    "Parallel(n_jobs=20)([delayed(process)(fp) \n",
    "                    for fp in glob('collections/msmarco-doc/collection_jsonl/msmarco-doc_*.jsonl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
