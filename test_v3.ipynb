{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:38:47.561348Z",
     "start_time": "2020-11-21T21:38:43.269329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10071876, 1)\n",
      "(327721,)\n",
      "rel    30.733081\n",
      "dtype: float64\n",
      "             rel\n",
      "qid pid         \n",
      "91  315007     0\n",
      "    777051     0\n",
      "    793527     1\n",
      "    932142     0\n",
      "    1156624    0\n",
      "    1378443    0\n",
      "    1480965    0\n",
      "    1649396    0\n",
      "    1662964    0\n",
      "    1779082    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 10071876 entries, (91, 315007) to (1185869, 8822597)\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   rel     int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 322.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import json\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from collections import defaultdict\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "from pyserini.ltr import *\n",
    "from pyserini.search import get_topics_with_reader\n",
    "\n",
    "def train_data_loader(neg_sample=30, random_seed=12345):\n",
    "    if os.path.exists(f'train_sampled_with_{neg_sample}_{random_seed}.pickle'):\n",
    "        sampled_train = pd.read_pickle(f'train_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        print(sampled_train.shape)\n",
    "        print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(sampled_train.groupby('qid').count().mean())\n",
    "        print(sampled_train.head(10))\n",
    "        print(sampled_train.info())\n",
    "        return sampled_train\n",
    "    else:\n",
    "        train = pd.read_csv('collections/msmarco-passage/qidpidtriples.train.full.tsv', sep=\"\\t\",\n",
    "                            names=['qid', 'pos_pid', 'neg_pid'], dtype=np.int32)\n",
    "        pos_half = train[['qid', 'pos_pid']].rename(columns={\"pos_pid\": \"pid\"}).drop_duplicates()\n",
    "        pos_half['rel'] = np.int32(1)\n",
    "        neg_half = train[['qid', 'neg_pid']].rename(columns={\"neg_pid\": \"pid\"}).drop_duplicates()\n",
    "        neg_half['rel'] = np.int32(0)\n",
    "        del train\n",
    "        sampled_neg_half = []\n",
    "        for qid, group in tqdm(neg_half.groupby('qid')):\n",
    "            sampled_neg_half.append(group.sample(n=min(neg_sample, len(group)), random_state=random_seed))\n",
    "        sampled_train = pd.concat([pos_half] + sampled_neg_half, axis=0, ignore_index=True)\n",
    "        sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "        print(sampled_train.shape)\n",
    "        print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(sampled_train.groupby('qid').count().mean())\n",
    "        print(sampled_train.head(10))\n",
    "        print(sampled_train.info())\n",
    "\n",
    "        sampled_train.to_pickle(f'train_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        return sampled_train\n",
    "sampled_train = train_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:38:48.559354Z",
     "start_time": "2020-11-21T21:38:47.563776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6668967, 1)\n",
      "(6980,)\n",
      "rel    955.439398\n",
      "dtype: float64\n",
      "            rel\n",
      "qid pid        \n",
      "2   10749     0\n",
      "    63138     0\n",
      "    96198     0\n",
      "    98589     0\n",
      "    98595     0\n",
      "    112123    0\n",
      "    112127    0\n",
      "    112128    0\n",
      "    112130    0\n",
      "    112131    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 6668967 entries, (2, 10749) to (1102400, 8837328)\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   rel     int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 253.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def dev_data_loader(task='rerank'):\n",
    "    if os.path.exists(f'dev_{task}.pickle'):\n",
    "        dev = pd.read_pickle(f'dev_{task}.pickle')\n",
    "        print(dev.shape)\n",
    "        print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(dev.groupby('qid').count().mean())\n",
    "        print(dev.head(10))\n",
    "        print(dev.info())\n",
    "        return dev\n",
    "    else:\n",
    "        if task == 'rerank':\n",
    "            dev = pd.read_csv('collections/msmarco-passage/top1000.dev', sep=\"\\t\",\n",
    "                              names=['qid', 'pid', 'query', 'doc'], usecols=['qid', 'pid'], dtype=np.int32)\n",
    "        elif task == 'anserini':\n",
    "            dev = pd.read_csv('runs/msmarco-passage/run.msmarco-passage.dev.small.tsv',sep=\"\\t\",\n",
    "                            names=['qid','pid','rank'], dtype=np.int32)\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "        dev_qrel = pd.read_csv('collections/msmarco-passage/qrels.dev.small.tsv', sep=\"\\t\",\n",
    "                               names=[\"qid\", \"q0\", \"pid\", \"rel\"], usecols=['qid', 'pid', 'rel'], dtype=np.int32)\n",
    "        dev = dev.merge(dev_qrel, left_on=['qid', 'pid'], right_on=['qid', 'pid'], how='left')\n",
    "        dev['rel'] = dev['rel'].fillna(0).astype(np.int32)\n",
    "        dev = dev.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "        \n",
    "        print(dev.shape)\n",
    "        print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(dev.groupby('qid').count().mean())\n",
    "        print(dev.head(10))\n",
    "        print(dev.info())\n",
    "\n",
    "        dev.to_pickle(f'dev_{task}.pickle')\n",
    "        return dev\n",
    "dev = dev_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:38:53.310794Z",
     "start_time": "2020-11-21T21:38:48.561154Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_loader(choice='default'):\n",
    "    if os.path.exists(f'query_{choice}_tokenized.pickle'):\n",
    "        return pickle.load(open(f'query_{choice}_tokenized.pickle','rb'))\n",
    "    else:\n",
    "        if choice == 'default':\n",
    "            analyzer = Analyzer(get_lucene_analyzer())\n",
    "            queries = get_topics_with_reader('io.anserini.search.topicreader.TsvIntTopicReader', \\\n",
    "                                             'collections/msmarco-passage/queries.train.tsv')\n",
    "            queries.update(get_topics_with_reader('io.anserini.search.topicreader.TsvIntTopicReader', \\\n",
    "                                                  'collections/msmarco-passage/queries.dev.tsv'))\n",
    "            for qid,value in queries.items():\n",
    "                assert 'tokenized' not in value\n",
    "                value['tokenized'] = analyzer.analyze(value['title'])\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "\n",
    "        pickle.dump(queries,open(f'query_{choice}_tokenized.pickle','wb'))\n",
    "\n",
    "        return queries\n",
    "queries = query_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:51:07.271486Z",
     "start_time": "2020-11-21T21:51:07.204857Z"
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureExtractor('indexes/msmarco-passage/lucene-index-msmarco/',max(multiprocessing.cpu_count()//2,1))\n",
    "fe.add(BM25(k1=0.9,b=0.4))\n",
    "fe.add(BM25(k1=1.2,b=0.75))\n",
    "fe.add(BM25(k1=2.0,b=0.75))\n",
    "\n",
    "fe.add(LMDir(mu=1000))\n",
    "fe.add(LMDir(mu=1500))\n",
    "fe.add(LMDir(mu=2500))\n",
    "\n",
    "fe.add(LMJM(0.1))\n",
    "fe.add(LMJM(0.4))\n",
    "fe.add(LMJM(0.7))\n",
    "\n",
    "# fe.add(NTFIDF())\n",
    "fe.add(ProbalitySum())\n",
    "\n",
    "fe.add(DFR_GL2())\n",
    "fe.add(DFR_In_expB2())\n",
    "fe.add(DPH())\n",
    "\n",
    "# fe.add(ContextDFR_GL2(AvgPooler()))\n",
    "# fe.add(ContextDFR_GL2(VarPooler()))\n",
    "# fe.add(ContextDFR_In_expB2(AvgPooler()))\n",
    "# fe.add(ContextDFR_In_expB2(VarPooler()))\n",
    "# fe.add(ContextDPH(AvgPooler()))\n",
    "# fe.add(ContextDPH(VarPooler()))\n",
    "\n",
    "fe.add(Proximity())\n",
    "fe.add(TPscore())\n",
    "fe.add(tpDist())\n",
    "fe.add(SDM())\n",
    "\n",
    "fe.add(DocSize())\n",
    "fe.add(Entropy())\n",
    "fe.add(StopCover())\n",
    "fe.add(StopRatio())\n",
    "\n",
    "fe.add(QueryLength())\n",
    "fe.add(QueryLengthNonStopWords())\n",
    "fe.add(QueryCoverageRatio())\n",
    "fe.add(UniqueTermCount())\n",
    "fe.add(MatchingTermCount())\n",
    "fe.add(SCS())\n",
    "\n",
    "fe.add(tfStat(AvgPooler()))\n",
    "fe.add(tfStat(SumPooler()))\n",
    "fe.add(tfStat(MinPooler()))\n",
    "fe.add(tfStat(MaxPooler()))\n",
    "fe.add(tfStat(VarPooler()))\n",
    "fe.add(tfIdfStat(AvgPooler()))\n",
    "fe.add(tfIdfStat(SumPooler()))\n",
    "fe.add(tfIdfStat(MinPooler()))\n",
    "fe.add(tfIdfStat(MaxPooler()))\n",
    "fe.add(tfIdfStat(VarPooler()))\n",
    "fe.add(scqStat(AvgPooler()))\n",
    "fe.add(scqStat(SumPooler()))\n",
    "fe.add(scqStat(MinPooler()))\n",
    "fe.add(scqStat(MaxPooler()))\n",
    "fe.add(scqStat(VarPooler()))\n",
    "fe.add(normalizedTfStat(AvgPooler()))\n",
    "fe.add(normalizedTfStat(SumPooler()))\n",
    "fe.add(normalizedTfStat(MinPooler()))\n",
    "fe.add(normalizedTfStat(MaxPooler()))\n",
    "fe.add(normalizedTfStat(VarPooler()))\n",
    "fe.add(normalizedDocSizeStat(AvgPooler()))\n",
    "fe.add(normalizedDocSizeStat(SumPooler()))\n",
    "fe.add(normalizedDocSizeStat(MinPooler()))\n",
    "fe.add(normalizedDocSizeStat(MaxPooler()))\n",
    "fe.add(normalizedDocSizeStat(VarPooler()))\n",
    "\n",
    "fe.add(idfStat(AvgPooler()))\n",
    "fe.add(idfStat(SumPooler()))\n",
    "fe.add(idfStat(MinPooler()))\n",
    "fe.add(idfStat(MaxPooler()))\n",
    "fe.add(idfStat(VarPooler()))\n",
    "fe.add(idfStat(MaxMinRatioPooler()))\n",
    "fe.add(idfStat(ConfidencePooler()))\n",
    "fe.add(ictfStat(AvgPooler()))\n",
    "fe.add(ictfStat(SumPooler()))\n",
    "fe.add(ictfStat(MinPooler()))\n",
    "fe.add(ictfStat(MaxPooler()))\n",
    "fe.add(ictfStat(VarPooler()))\n",
    "fe.add(ictfStat(MaxMinRatioPooler()))\n",
    "fe.add(ictfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(UnorderedSequentialPairs(3))\n",
    "fe.add(UnorderedSequentialPairs(8))\n",
    "fe.add(UnorderedSequentialPairs(15))\n",
    "fe.add(OrderedSequentialPairs(3))\n",
    "fe.add(OrderedSequentialPairs(8))\n",
    "fe.add(OrderedSequentialPairs(15))\n",
    "fe.add(UnorderedQueryPairs(3))\n",
    "fe.add(UnorderedQueryPairs(8))\n",
    "fe.add(UnorderedQueryPairs(15))\n",
    "fe.add(OrderedQueryPairs(3))\n",
    "fe.add(OrderedQueryPairs(8))\n",
    "fe.add(OrderedQueryPairs(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:51:10.344639Z",
     "start_time": "2020-11-21T21:51:10.319289Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract(df, queries, fe):\n",
    "    df_pieces = []\n",
    "    fetch_later = []\n",
    "    qidpid2rel = defaultdict(dict)\n",
    "    need_rows = 0\n",
    "    for qid,group in tqdm(df.groupby('qid')):\n",
    "        for t in group.reset_index().itertuples():\n",
    "            assert t.pid not in qidpid2rel[t.qid]\n",
    "            qidpid2rel[t.qid][t.pid] = t.rel\n",
    "            need_rows += 1\n",
    "        fe.lazy_extract(str(qid),queries[qid]['tokenized'],list(qidpid2rel[t.qid].keys()))\n",
    "        fetch_later.append(str(qid))\n",
    "        if len(fetch_later) == 10000:\n",
    "            info = np.zeros(shape=(need_rows,3), dtype=np.int32)\n",
    "            feature = np.zeros(shape=(need_rows,len(fe.feature_names())), dtype=np.float32)\n",
    "            idx = 0\n",
    "            for qid in fetch_later:\n",
    "                for doc in fe.get_result(qid):\n",
    "                    info[idx,0] = int(qid)\n",
    "                    info[idx,1] = int(doc['pid'])\n",
    "                    info[idx,2] = qidpid2rel[int(qid)][int(doc['pid'])]\n",
    "                    feature[idx,:] = doc['features']\n",
    "                    idx += 1\n",
    "            info = pd.DataFrame(info, columns=['qid','pid','rel'])\n",
    "            feature = pd.DataFrame(feature, columns=fe.feature_names())\n",
    "            df_pieces.append(pd.concat([info,feature], axis=1))\n",
    "            fetch_later = []\n",
    "            need_rows = 0\n",
    "    #deal with rest\n",
    "    if len(fetch_later) > 0:\n",
    "        info = np.zeros(shape=(need_rows,3), dtype=np.int32)\n",
    "        feature = np.zeros(shape=(need_rows,len(fe.feature_names())), dtype=np.float32)\n",
    "        idx = 0\n",
    "        for qid in fetch_later:\n",
    "            for doc in fe.get_result(qid):\n",
    "                info[idx,0] = int(qid)\n",
    "                info[idx,1] = int(doc['pid'])\n",
    "                info[idx,2] = qidpid2rel[int(qid)][int(doc['pid'])]\n",
    "                feature[idx,:] = doc['features']\n",
    "                idx += 1\n",
    "        info = pd.DataFrame(info, columns=['qid','pid','rel'])\n",
    "        feature = pd.DataFrame(feature, columns=fe.feature_names())\n",
    "        df_pieces.append(pd.concat([info,feature], axis=1))\n",
    "    data = pd.concat(df_pieces, axis=0, ignore_index=True)\n",
    "    data = data.sort_values(by='qid', kind='mergesort')\n",
    "    group = data.groupby('qid').agg(count=('pid', 'count'))['count']\n",
    "    return data,group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:51:11.126850Z",
     "start_time": "2020-11-21T21:51:11.115731Z"
    }
   },
   "outputs": [],
   "source": [
    "def hash_df(df):\n",
    "    h = pd.util.hash_pandas_object(df)\n",
    "    return hex(h.sum().astype(np.uint64))\n",
    "\n",
    "\n",
    "def hash_anserini_jar():\n",
    "    find = glob.glob(os.environ['ANSERINI_CLASSPATH'] + \"/*fatjar.jar\")\n",
    "    assert len(find) == 1\n",
    "    md5Hash = hashlib.md5(open(find[0], 'rb').read())\n",
    "    return md5Hash.hexdigest()\n",
    "\n",
    "\n",
    "def hash_fe(fe):\n",
    "    return hashlib.md5(','.join(sorted(fe.feature_names())).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def data_loader(task, df, queries, fe):\n",
    "    df_hash = hash_df(df)\n",
    "    jar_hash = hash_anserini_jar()\n",
    "    fe_hash = hash_fe(fe)\n",
    "    if os.path.exists(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle'):\n",
    "        res = pickle.load(open(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle','rb'))\n",
    "        print(res['data'].shape)\n",
    "        print(res['data'].qid.drop_duplicates().shape)\n",
    "        print(res['group'].mean())\n",
    "        print(res['data'].head(10))\n",
    "        print(res['data'].info())\n",
    "        return res\n",
    "    else:\n",
    "        if task == 'train' or task == 'dev': \n",
    "            data,group = extract(df, queries, fe)\n",
    "            obj = {'data':data,'group':group,'df_hash':df_hash,'jar_hash':jar_hash,'fe_hash':fe_hash}\n",
    "            print(data.shape)\n",
    "            print(data.qid.drop_duplicates().shape)\n",
    "            print(group.mean())\n",
    "            print(data.head(10))\n",
    "            print(data.info())\n",
    "            pickle.dump(obj,open(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle','wb'))\n",
    "            return obj\n",
    "        else:\n",
    "            raise Exception('unknown parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def export(df, analyzer, fn):\n",
    "    with open(fn,'w') as f:\n",
    "        for qid,group in df.groupby('qid'):\n",
    "            line = {}\n",
    "            line['qid'] = qid\n",
    "            line['queryTokens'] = queries[qid]['tokenized']\n",
    "            line['docIds'] = [str(did) for did in group['pid'].drop_duplicates().tolist()]\n",
    "            f.write(json.dumps(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T21:52:10.180082Z",
     "start_time": "2020-11-21T21:51:11.707605Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9999/327721 [00:43<23:11, 228.29it/s] \n"
     ]
    },
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: java.lang.NullPointerException java.util.concurrent.ExecutionException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-905694d27c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_extracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_extracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0msampled_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-9015f0f10ca9>\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(task, df, queries, fe)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dev'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'df_hash'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'jar_hash'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mjar_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fe_hash'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfe_hash\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-969d794de479>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(df, queries, fe)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_later\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/zhangyue/pyserini/pyserini/ltr/_base.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, qid)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.JavaMethod.call_method\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mjnius/jnius_jvm_dlopen.pxi\u001b[0m in \u001b[0;36mjnius.create_jnienv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: java.lang.NullPointerException java.util.concurrent.ExecutionException"
     ]
    }
   ],
   "source": [
    "train_extracted = data_loader('train', sampled_train, queries, fe)\n",
    "dev_extracted = data_loader('dev', dev, queries, fe)\n",
    "del sampled_train, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_name = fe.feature_names()\n",
    "train_X = train_extracted['data'].loc[:, feature_name]\n",
    "train_Y = train_extracted['data']['rel']\n",
    "dev_X = dev_extracted['data'].loc[:, feature_name]\n",
    "dev_Y = dev_extracted['data']['rel']\n",
    "lgb_train = lgb.Dataset(train_X,label=train_Y,group=train_extracted['group'])\n",
    "lgb_valid = lgb.Dataset(dev_X,label=dev_Y,group=dev_extracted['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_mrr(dev_data):\n",
    "    score_tie_counter = 0\n",
    "    score_tie_query = set()\n",
    "\n",
    "    MRR = []\n",
    "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
    "        group = group.reset_index()\n",
    "        rank = 0\n",
    "        prev_score = None\n",
    "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
    "        # stable sort is also used in LightGBM\n",
    "\n",
    "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
    "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
    "                score_tie_counter += 1\n",
    "                score_tie_query.add(qid)\n",
    "            prev_score = t.score\n",
    "            prev_pid = t.pid\n",
    "            rank += 1\n",
    "            if t.rel>0:\n",
    "                MRR.append(1.0/rank)\n",
    "                break\n",
    "            elif rank == 10 or rank == len(group):\n",
    "                MRR.append(0.)\n",
    "                break\n",
    "\n",
    "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
    "    print(score_tie,np.mean(MRR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total groups: 327721, total data: 32429423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.345941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9841\n",
      "[LightGBM] [Info] Number of data points in the train set: 32429423, number of used features: 55\n",
      "[LightGBM] [Info] Total groups: 6980, total data: 6668967\n",
      "0.38917278770788793\n",
      "873\n",
      "[('DFR_In_expB2', 2578), ('SCQvar', 2220), ('SCQavg', 2163), ('DocSize', 1971), ('IDFsum', 1889), ('NormalizedTFvar', 1870), ('NormalizedTFmin', 1696), ('TFIDFvar', 1623), ('SCQmax', 1613), ('SCQsum', 1563), ('TFIDFmax', 1562), ('NormalizedTFavg', 1529), ('Proximity', 1513), ('DFR_GL2', 1485), ('BM25_k1_2.00_b_0.75', 1405), ('TFvar', 1393), ('BM25_k1_0.90_b_0.40', 1305), ('LMD_mu_2500', 1272), ('TFIDFavg', 1237), ('TFIDFsum', 1167), ('SCS', 999), ('UnorderedQueryPairs15', 933), ('ICTFvar', 920), ('IDFavg', 891), ('IDFvar', 868), ('TFavg', 842), ('IDFmax', 837), ('BM25_k1_1.20_b_0.75', 832), ('ICTFavg', 827), ('ICTFmax', 824), ('UnorderedSequentialPairs15', 791), ('ICTFsum', 778), ('UnorderedQueryPairs8', 722), ('LMD_mu_1000', 716), ('UnorderedQueryPairs3', 709), ('TFsum', 683), ('OrderedQueryPairs15', 617), ('OrderedQueryPairs3', 615), ('LMD_mu_1500', 589), ('QueryCoverageRatio', 576), ('OrderedQueryPairs8', 554), ('OrderedSequentialPairs3', 522), ('MatchingTermCount', 498), ('LMJM_lambda_0.70', 491), ('UnorderedSequentialPairs8', 477), ('UnorderedSequentialPairs3', 470), ('NormalizedTFsum', 442), ('OrderedSequentialPairs8', 408), ('OrderedSequentialPairs15', 393), ('LMJM_lambda_0.10', 386), ('TFmax', 368), ('LMJM_lambda_0.40', 304), ('UniqueQueryTerms', 107), ('QueryLength', 65), ('DPH', 18), ('TFmin', 0), ('TFIDFmin', 0), ('NormalizedTFmax', 0), ('IDFmin', 0), ('ICTFmin', 0), ('SCQmin', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:36<00:00, 192.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 936 times in 712 queries 0.21424682767089642\n",
      "[LightGBM] [Info] Total groups: 327721, total data: 32429423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.322068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9841\n",
      "[LightGBM] [Info] Number of data points in the train set: 32429423, number of used features: 55\n",
      "[LightGBM] [Info] Total groups: 6980, total data: 6668967\n",
      "0.38958647913160804\n",
      "316\n",
      "[('DFR_In_expB2', 1170), ('IDFsum', 1004), ('SCQavg', 856), ('SCQsum', 746), ('NormalizedTFmin', 697), ('LMD_mu_2500', 676), ('NormalizedTFvar', 673), ('DocSize', 669), ('DFR_GL2', 633), ('SCQvar', 614), ('BM25_k1_0.90_b_0.40', 596), ('BM25_k1_2.00_b_0.75', 592), ('TFIDFvar', 591), ('TFIDFmax', 558), ('UnorderedQueryPairs15', 463), ('TFvar', 414), ('MatchingTermCount', 406), ('SCQmax', 404), ('NormalizedTFavg', 385), ('TFsum', 368), ('UnorderedQueryPairs3', 349), ('QueryCoverageRatio', 329), ('Proximity', 316), ('TFavg', 316), ('UnorderedSequentialPairs15', 315), ('UnorderedQueryPairs8', 315), ('OrderedSequentialPairs3', 294), ('OrderedQueryPairs3', 289), ('TFIDFsum', 278), ('OrderedQueryPairs8', 265), ('LMD_mu_1000', 262), ('BM25_k1_1.20_b_0.75', 251), ('TFIDFavg', 251), ('LMD_mu_1500', 226), ('OrderedQueryPairs15', 213), ('IDFavg', 207), ('IDFmax', 206), ('SCS', 202), ('ICTFsum', 202), ('IDFvar', 191), ('UnorderedSequentialPairs3', 188), ('ICTFmax', 169), ('ICTFvar', 161), ('LMJM_lambda_0.70', 160), ('TFmax', 153), ('ICTFavg', 153), ('UnorderedSequentialPairs8', 148), ('OrderedSequentialPairs8', 144), ('NormalizedTFsum', 134), ('LMJM_lambda_0.10', 124), ('OrderedSequentialPairs15', 117), ('LMJM_lambda_0.40', 108), ('QueryLength', 14), ('UniqueQueryTerms', 14), ('DPH', 13), ('TFmin', 0), ('TFIDFmin', 0), ('NormalizedTFmax', 0), ('IDFmin', 0), ('ICTFmin', 0), ('SCQmin', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:39<00:00, 177.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 892 times in 699 queries 0.21428753354254784\n",
      "[LightGBM] [Info] Total groups: 327721, total data: 32429423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.099861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9841\n",
      "[LightGBM] [Info] Number of data points in the train set: 32429423, number of used features: 55\n",
      "[LightGBM] [Info] Total groups: 6980, total data: 6668967\n",
      "0.3905145464744318\n",
      "472\n",
      "[('DFR_In_expB2', 1647), ('IDFsum', 1222), ('SCQavg', 1157), ('SCQvar', 1060), ('NormalizedTFmin', 993), ('DocSize', 975), ('SCQsum', 975), ('NormalizedTFvar', 939), ('TFIDFvar', 914), ('LMD_mu_2500', 897), ('BM25_k1_2.00_b_0.75', 861), ('DFR_GL2', 844), ('TFIDFmax', 819), ('BM25_k1_0.90_b_0.40', 817), ('TFvar', 710), ('NormalizedTFavg', 705), ('SCQmax', 704), ('UnorderedQueryPairs15', 642), ('Proximity', 636), ('TFIDFsum', 555), ('TFIDFavg', 502), ('TFavg', 497), ('TFsum', 476), ('UnorderedSequentialPairs15', 464), ('UnorderedQueryPairs8', 437), ('UnorderedQueryPairs3', 428), ('LMD_mu_1000', 410), ('MatchingTermCount', 407), ('IDFavg', 404), ('SCS', 396), ('ICTFsum', 389), ('BM25_k1_1.20_b_0.75', 386), ('OrderedSequentialPairs3', 359), ('OrderedQueryPairs3', 357), ('ICTFvar', 356), ('IDFvar', 355), ('IDFmax', 354), ('LMD_mu_1500', 352), ('QueryCoverageRatio', 349), ('ICTFmax', 348), ('OrderedQueryPairs8', 347), ('OrderedQueryPairs15', 345), ('ICTFavg', 339), ('UnorderedSequentialPairs3', 299), ('LMJM_lambda_0.70', 270), ('NormalizedTFsum', 263), ('OrderedSequentialPairs8', 241), ('LMJM_lambda_0.10', 221), ('UnorderedSequentialPairs8', 218), ('TFmax', 201), ('LMJM_lambda_0.40', 181), ('OrderedSequentialPairs15', 177), ('UniqueQueryTerms', 30), ('QueryLength', 23), ('DPH', 11), ('TFmin', 0), ('TFIDFmin', 0), ('NormalizedTFmax', 0), ('IDFmin', 0), ('ICTFmin', 0), ('SCQmin', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:37<00:00, 185.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 873 times in 686 queries 0.2147775390003184\n",
      "[LightGBM] [Info] Total groups: 327721, total data: 32429423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.174163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9841\n",
      "[LightGBM] [Info] Number of data points in the train set: 32429423, number of used features: 55\n",
      "[LightGBM] [Info] Total groups: 6980, total data: 6668967\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'max_bin':255,\n",
    "    'num_leaves':63,\n",
    "    'max_depth':10,\n",
    "    'min_data_in_leaf':50,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':20,\n",
    "    'feature_fraction':1,\n",
    "    'learning_rate':0.1,\n",
    "    'num_boost_round':1000,\n",
    "    'early_stopping_round':500,\n",
    "    'metric':['map'],\n",
    "    'eval_at':[10],\n",
    "    'label_gain':[0,1],\n",
    "    'lambdarank_truncation_level':20,\n",
    "    'num_threads':max(multiprocessing.cpu_count()//2,1)\n",
    "}\n",
    "num_boost_round = params.pop('num_boost_round')\n",
    "early_stopping_round = params.pop('early_stopping_round')\n",
    "eval_results={}\n",
    "dev_extracted['data']['score']=0.\n",
    "for seed in [12345,31345,21356,65743,68786]:\n",
    "    params['seed'] = seed\n",
    "    gbm = lgb.train(params, lgb_train, \n",
    "                    valid_sets=lgb_valid,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    early_stopping_rounds =early_stopping_round,\n",
    "                    feature_name=feature_name,\n",
    "                    evals_result=eval_results,\n",
    "                    verbose_eval=False)\n",
    "    dev_extracted['data']['score'] += gbm.predict(dev_X)\n",
    "    best_score = gbm.best_score['valid_0']['map@10']\n",
    "    print(best_score)\n",
    "    best_iteration = gbm.best_iteration\n",
    "    print(best_iteration)\n",
    "    eval_map = eval_results['valid_0']['map@10']\n",
    "    # print(eval_map)\n",
    "    feature_importances = sorted(list(zip(feature_name,gbm.feature_importance().tolist())),key=lambda x:x[1],reverse=True)\n",
    "    print(feature_importances)\n",
    "    eval_mrr(dev_extracted['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'max_bin':255,\n",
    "    'num_leaves':63,\n",
    "    'max_depth':10,\n",
    "    'min_data_in_leaf':50,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':50,\n",
    "    'feature_fraction':1,\n",
    "    'learning_rate':0.1,\n",
    "    'num_boost_round':500,\n",
    "    'metric':['map'],\n",
    "    'eval_at':[10],\n",
    "    'label_gain':[0,1],\n",
    "    'lambdarank_truncation_level':20,\n",
    "    'seed':12345,\n",
    "    'num_threads':max(multiprocessing.cpu_count()//2,1)\n",
    "}\n",
    "\n",
    "num_boost_round = params.pop('num_boost_round')\n",
    "eval_results={}\n",
    "cv_gbm = lgb.cv(params, lgb_train, nfold=5, \n",
    "                num_boost_round=num_boost_round,\n",
    "                feature_name=feature_name,\n",
    "                verbose_eval=False,\n",
    "                return_cvbooster=True)\n",
    "dev_extracted['data']['score'] = 0.\n",
    "for gbm in cv_gbm['cvbooster'].boosters:\n",
    "    dev_extracted['data']['score']+=gbm.predict(dev_X)\n",
    "feature_importances = sorted(list(zip(feature_name,gbm.feature_importance().tolist())),key=lambda x:x[1],reverse=True)\n",
    "print(feature_importances)\n",
    "eval_mrr(dev_extracted['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
