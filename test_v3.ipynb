{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:03:57.427814Z",
     "start_time": "2020-12-10T19:03:56.524015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4416413, 1)\n",
      "(400782,)\n",
      "rel    11.019489\n",
      "dtype: float64\n",
      "             rel\n",
      "qid pid         \n",
      "3   970816     0\n",
      "    1142680    1\n",
      "    2019206    0\n",
      "    2605131    0\n",
      "    2963098    0\n",
      "    2971685    0\n",
      "    3783924    0\n",
      "    5067083    0\n",
      "    5904778    0\n",
      "    6176208    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4416413 entries, (3, 970816) to (1185869, 7770561)\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   rel     int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 166.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import json\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from collections import defaultdict\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "from pyserini.ltr import *\n",
    "from pyserini.search import get_topics_with_reader\n",
    "\n",
    "def train_data_loader(task='triple', neg_sample=10, random_seed=12345):\n",
    "    if os.path.exists(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle'):\n",
    "        sampled_train = pd.read_pickle(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        print(sampled_train.shape)\n",
    "        print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(sampled_train.groupby('qid').count().mean())\n",
    "        print(sampled_train.head(10))\n",
    "        print(sampled_train.info())\n",
    "        return sampled_train\n",
    "    else:\n",
    "        if task == 'triple':\n",
    "            train = pd.read_csv('collections/msmarco-passage/qidpidtriples.train.full.2.tsv', sep=\"\\t\",\n",
    "                                names=['qid', 'pos_pid', 'neg_pid'], dtype=np.int32)\n",
    "            pos_half = train[['qid', 'pos_pid']].rename(columns={\"pos_pid\": \"pid\"}).drop_duplicates()\n",
    "            pos_half['rel'] = np.int32(1)\n",
    "            neg_half = train[['qid', 'neg_pid']].rename(columns={\"neg_pid\": \"pid\"}).drop_duplicates()\n",
    "            neg_half['rel'] = np.int32(0)\n",
    "            del train\n",
    "            sampled_neg_half = []\n",
    "            for qid, group in tqdm(neg_half.groupby('qid')):\n",
    "                sampled_neg_half.append(group.sample(n=min(neg_sample, len(group)), random_state=random_seed))\n",
    "            sampled_train = pd.concat([pos_half] + sampled_neg_half, axis=0, ignore_index=True)\n",
    "            sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "            print(sampled_train.shape)\n",
    "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "            print(sampled_train.groupby('qid').count().mean())\n",
    "            print(sampled_train.head(10))\n",
    "            print(sampled_train.info())\n",
    "\n",
    "            sampled_train.to_pickle(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        elif task == 'rank':\n",
    "            qrel = defaultdict(list)\n",
    "            with open(\"collections/msmarco-passage/qrels.train.tsv\") as f:\n",
    "                for line in f:\n",
    "                    topicid, _, docid, rel = line.strip().split('\\t')\n",
    "                    assert rel == \"1\", line.split(' ')\n",
    "                    qrel[topicid].append(docid)\n",
    "            \n",
    "            qid2pos = defaultdict(list)\n",
    "            qid2neg = defaultdict(list)\n",
    "            with open(\"runs/msmarco-passage/run.train.small.tsv\") as f:\n",
    "                for line in tqdm(f):\n",
    "                    topicid, docid, rank = line.split()\n",
    "                    assert topicid in qrel\n",
    "                    if docid in qrel[topicid]:\n",
    "                        qid2pos[topicid].append(docid)\n",
    "                    else:\n",
    "                        qid2neg[topicid].append(docid)\n",
    "            sampled_train = []\n",
    "            for topicid, pos_list in tqdm(qid2pos.items()):\n",
    "                neg_list = random.sample(qid2neg[topicid], min(len(qid2neg[topicid]), neg_sample))\n",
    "                for positive_docid in pos_list:\n",
    "                    sampled_train.append((int(topicid), int(positive_docid), 1))\n",
    "                for negative_docid in neg_list:\n",
    "                    sampled_train.append((int(topicid), int(negative_docid), 0))\n",
    "            sampled_train = pd.DataFrame(sampled_train,columns=['qid','pid','rel'],dtype=np.int32)\n",
    "            sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "            print(sampled_train.shape)\n",
    "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "            print(sampled_train.groupby('qid').count().mean())\n",
    "            print(sampled_train.head(10))\n",
    "            print(sampled_train.info())\n",
    "\n",
    "            sampled_train.to_pickle(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "        return sampled_train\n",
    "sampled_train = train_data_loader(task='triple', neg_sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:03:58.870569Z",
     "start_time": "2020-12-10T19:03:57.429763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6974598, 2)\n",
      "(6980,)\n",
      "rank    999.226074\n",
      "rel     999.226074\n",
      "dtype: float64\n",
      "            rank  rel\n",
      "qid pid              \n",
      "2   55860    345    0\n",
      "    72202    557    0\n",
      "    72210    213    0\n",
      "    98589    278    0\n",
      "    98590    323    0\n",
      "    98593    580    0\n",
      "    98595    553    0\n",
      "    112123   108    0\n",
      "    112126   469    0\n",
      "    112127    21    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 6974598 entries, (2, 55860) to (1102400, 8830447)\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   rank    int32\n",
      " 1   rel     int32\n",
      "dtypes: int32(2)\n",
      "memory usage: 282.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def dev_data_loader(task='pygaggle'):\n",
    "    if os.path.exists(f'dev_{task}.pickle'):\n",
    "        dev = pd.read_pickle(f'dev_{task}.pickle')\n",
    "        print(dev.shape)\n",
    "        print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(dev.groupby('qid').count().mean())\n",
    "        print(dev.head(10))\n",
    "        print(dev.info())\n",
    "        dev_qrel = pd.read_pickle(f'dev_qrel.pickle')\n",
    "        return dev, dev_qrel\n",
    "    else:\n",
    "        if task == 'rerank':\n",
    "            dev = pd.read_csv('collections/msmarco-passage/top1000.dev', sep=\"\\t\",\n",
    "                              names=['qid', 'pid', 'query', 'doc'], usecols=['qid', 'pid'], dtype=np.int32)\n",
    "        elif task == 'anserini':\n",
    "            dev = pd.read_csv('runs/msmarco-passage/run.msmarco-passage.dev.small.tsv',sep=\"\\t\",\n",
    "                            names=['qid','pid','rank'], dtype=np.int32)\n",
    "        elif task == 'pygaggle':\n",
    "            dev = pd.read_csv('../pygaggle/data/msmarco_ans_entire/run.dev.small.tsv',sep=\"\\t\",\n",
    "                            names=['qid','pid','rank'], dtype=np.int32)\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "        dev_qrel = pd.read_csv('collections/msmarco-passage/qrels.dev.small.tsv', sep=\"\\t\",\n",
    "                               names=[\"qid\", \"q0\", \"pid\", \"rel\"], usecols=['qid', 'pid', 'rel'], dtype=np.int32)\n",
    "        dev = dev.merge(dev_qrel, left_on=['qid', 'pid'], right_on=['qid', 'pid'], how='left')\n",
    "        dev['rel'] = dev['rel'].fillna(0).astype(np.int32)\n",
    "        dev = dev.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "        \n",
    "        print(dev.shape)\n",
    "        print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(dev.groupby('qid').count().mean())\n",
    "        print(dev.head(10))\n",
    "        print(dev.info())\n",
    "\n",
    "        dev.to_pickle(f'dev_{task}.pickle')\n",
    "        dev_qrel.to_pickle(f'dev_qrel.pickle')\n",
    "        return dev, dev_qrel\n",
    "dev, dev_qrel = dev_data_loader(task='pygaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:11.744022Z",
     "start_time": "2020-12-10T19:03:58.872612Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_loader(choice='default'):\n",
    "    if os.path.exists(f'query_{choice}_tokenized.pickle'):\n",
    "        return pickle.load(open(f'query_{choice}_tokenized.pickle','rb'))\n",
    "    else:\n",
    "        if choice == 'default':\n",
    "            analyzer = Analyzer(get_lucene_analyzer())\n",
    "            nonStopAnalyzer = Analyzer(get_lucene_analyzer(stopwords=False))\n",
    "            queries = get_topics_with_reader('io.anserini.search.topicreader.TsvIntTopicReader', \\\n",
    "                                             'collections/msmarco-passage/queries.train.tsv')\n",
    "            queries.update(get_topics_with_reader('io.anserini.search.topicreader.TsvIntTopicReader', \\\n",
    "                                                  'collections/msmarco-passage/queries.dev.tsv'))\n",
    "            for qid,value in queries.items():\n",
    "                assert 'tokenized' not in value\n",
    "                value['tokenized'] = analyzer.analyze(value['title'])\n",
    "                assert 'nonSW' not in value\n",
    "                value['nonSW'] = nonStopAnalyzer.analyze(value['title'])\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "\n",
    "        pickle.dump(queries,open(f'query_{choice}_tokenized.pickle','wb'))\n",
    "\n",
    "        return queries\n",
    "queries = query_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:12.954853Z",
     "start_time": "2020-12-10T19:04:11.747531Z"
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureExtractor('indexes/msmarco-passage/lucene-index-msmarco/',max(multiprocessing.cpu_count()//2,1))\n",
    "fe.add(BM25(k1=0.9,b=0.4))\n",
    "fe.add(BM25(k1=1.2,b=0.75))\n",
    "fe.add(BM25(k1=2.0,b=0.75))\n",
    "\n",
    "fe.add(LMDir(mu=1000))\n",
    "fe.add(LMDir(mu=1500))\n",
    "fe.add(LMDir(mu=2500))\n",
    "\n",
    "fe.add(LMJM(0.1))\n",
    "fe.add(LMJM(0.4))\n",
    "fe.add(LMJM(0.7))\n",
    "\n",
    "fe.add(NTFIDF())\n",
    "fe.add(ProbalitySum())\n",
    "\n",
    "fe.add(DFR_GL2())\n",
    "fe.add(DFR_In_expB2())\n",
    "fe.add(DPH())\n",
    "\n",
    "# fe.add(ContextDFR_GL2(AvgPooler()))\n",
    "# fe.add(ContextDFR_GL2(VarPooler()))\n",
    "# fe.add(ContextDFR_In_expB2(AvgPooler()))\n",
    "# fe.add(ContextDFR_In_expB2(VarPooler()))\n",
    "# fe.add(ContextDPH(AvgPooler()))\n",
    "# fe.add(ContextDPH(VarPooler()))\n",
    "\n",
    "fe.add(Proximity())\n",
    "fe.add(TPscore())\n",
    "fe.add(tpDist())\n",
    "# fe.add(SDM())\n",
    "\n",
    "fe.add(DocSize())\n",
    "fe.add(Entropy())\n",
    "fe.add(StopCover())\n",
    "fe.add(StopRatio())\n",
    "\n",
    "fe.add(QueryLength())\n",
    "fe.add(QueryLengthNonStopWords())\n",
    "fe.add(QueryCoverageRatio())\n",
    "fe.add(UniqueTermCount())\n",
    "fe.add(MatchingTermCount())\n",
    "fe.add(SCS())\n",
    "\n",
    "fe.add(tfStat(AvgPooler()))\n",
    "fe.add(tfStat(SumPooler()))\n",
    "fe.add(tfStat(MinPooler()))\n",
    "fe.add(tfStat(MaxPooler()))\n",
    "fe.add(tfStat(VarPooler()))\n",
    "fe.add(tfIdfStat(AvgPooler()))\n",
    "fe.add(tfIdfStat(SumPooler()))\n",
    "fe.add(tfIdfStat(MinPooler()))\n",
    "fe.add(tfIdfStat(MaxPooler()))\n",
    "fe.add(tfIdfStat(VarPooler()))\n",
    "fe.add(scqStat(AvgPooler()))\n",
    "fe.add(scqStat(SumPooler()))\n",
    "fe.add(scqStat(MinPooler()))\n",
    "fe.add(scqStat(MaxPooler()))\n",
    "fe.add(scqStat(VarPooler()))\n",
    "fe.add(normalizedTfStat(AvgPooler()))\n",
    "fe.add(normalizedTfStat(SumPooler()))\n",
    "fe.add(normalizedTfStat(MinPooler()))\n",
    "fe.add(normalizedTfStat(MaxPooler()))\n",
    "fe.add(normalizedTfStat(VarPooler()))\n",
    "# fe.add(normalizedDocSizeStat(AvgPooler()))\n",
    "# fe.add(normalizedDocSizeStat(SumPooler()))\n",
    "# fe.add(normalizedDocSizeStat(MinPooler()))\n",
    "# fe.add(normalizedDocSizeStat(MaxPooler()))\n",
    "# fe.add(normalizedDocSizeStat(VarPooler()))\n",
    "\n",
    "fe.add(idfStat(AvgPooler()))\n",
    "fe.add(idfStat(SumPooler()))\n",
    "fe.add(idfStat(MinPooler()))\n",
    "fe.add(idfStat(MaxPooler()))\n",
    "fe.add(idfStat(VarPooler()))\n",
    "fe.add(idfStat(MaxMinRatioPooler()))\n",
    "fe.add(idfStat(ConfidencePooler()))\n",
    "fe.add(ictfStat(AvgPooler()))\n",
    "fe.add(ictfStat(SumPooler()))\n",
    "fe.add(ictfStat(MinPooler()))\n",
    "fe.add(ictfStat(MaxPooler()))\n",
    "fe.add(ictfStat(VarPooler()))\n",
    "fe.add(ictfStat(MaxMinRatioPooler()))\n",
    "fe.add(ictfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(UnorderedSequentialPairs(3))\n",
    "fe.add(UnorderedSequentialPairs(8))\n",
    "fe.add(UnorderedSequentialPairs(15))\n",
    "fe.add(OrderedSequentialPairs(3))\n",
    "fe.add(OrderedSequentialPairs(8))\n",
    "fe.add(OrderedSequentialPairs(15))\n",
    "fe.add(UnorderedQueryPairs(3))\n",
    "fe.add(UnorderedQueryPairs(8))\n",
    "fe.add(UnorderedQueryPairs(15))\n",
    "fe.add(OrderedQueryPairs(3))\n",
    "fe.add(OrderedQueryPairs(8))\n",
    "fe.add(OrderedQueryPairs(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:12.971009Z",
     "start_time": "2020-12-10T19:04:12.957567Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract(df, queries, fe):\n",
    "    df_pieces = []\n",
    "    fetch_later = []\n",
    "    qidpid2rel = defaultdict(dict)\n",
    "    need_rows = 0\n",
    "    for qid,group in tqdm(df.groupby('qid')):\n",
    "        for t in group.reset_index().itertuples():\n",
    "            assert t.pid not in qidpid2rel[t.qid]\n",
    "            qidpid2rel[t.qid][t.pid] = t.rel\n",
    "            need_rows += 1\n",
    "        #test.py has bug here, it does not convert pid to str, not sure why it does not cause problem in java\n",
    "        fe.lazy_extract(str(qid),\n",
    "                        queries[qid]['nonSW'], \n",
    "                        queries[qid]['tokenized'],\n",
    "                        [str(pid) for pid in qidpid2rel[t.qid].keys()])\n",
    "        fetch_later.append(str(qid))\n",
    "        if len(fetch_later) == 10000:\n",
    "            info = np.zeros(shape=(need_rows,3), dtype=np.int32)\n",
    "            feature = np.zeros(shape=(need_rows,len(fe.feature_names())), dtype=np.float32)\n",
    "            idx = 0\n",
    "            for qid in fetch_later:\n",
    "                for doc in fe.get_result(qid):\n",
    "                    info[idx,0] = int(qid)\n",
    "                    info[idx,1] = int(doc['pid'])\n",
    "                    info[idx,2] = qidpid2rel[int(qid)][int(doc['pid'])]\n",
    "                    feature[idx,:] = doc['features']\n",
    "                    idx += 1\n",
    "            info = pd.DataFrame(info, columns=['qid','pid','rel'])\n",
    "            feature = pd.DataFrame(feature, columns=fe.feature_names())\n",
    "            df_pieces.append(pd.concat([info,feature], axis=1))\n",
    "            fetch_later = []\n",
    "            need_rows = 0\n",
    "    #deal with rest\n",
    "    if len(fetch_later) > 0:\n",
    "        info = np.zeros(shape=(need_rows,3), dtype=np.int32)\n",
    "        feature = np.zeros(shape=(need_rows,len(fe.feature_names())), dtype=np.float32)\n",
    "        idx = 0\n",
    "        for qid in fetch_later:\n",
    "            for doc in fe.get_result(qid):\n",
    "                info[idx,0] = int(qid)\n",
    "                info[idx,1] = int(doc['pid'])\n",
    "                info[idx,2] = qidpid2rel[int(qid)][int(doc['pid'])]\n",
    "                feature[idx,:] = doc['features']\n",
    "                idx += 1\n",
    "        info = pd.DataFrame(info, columns=['qid','pid','rel'])\n",
    "        feature = pd.DataFrame(feature, columns=fe.feature_names())\n",
    "        df_pieces.append(pd.concat([info,feature], axis=1))\n",
    "    data = pd.concat(df_pieces, axis=0, ignore_index=True)\n",
    "    data = data.sort_values(by='qid', kind='mergesort')\n",
    "    group = data.groupby('qid').agg(count=('pid', 'count'))['count']\n",
    "    return data,group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:12.983123Z",
     "start_time": "2020-12-10T19:04:12.972763Z"
    }
   },
   "outputs": [],
   "source": [
    "def hash_df(df):\n",
    "    h = pd.util.hash_pandas_object(df)\n",
    "    return hex(h.sum().astype(np.uint64))\n",
    "\n",
    "\n",
    "def hash_anserini_jar():\n",
    "    find = glob.glob(os.environ['ANSERINI_CLASSPATH'] + \"/*fatjar.jar\")\n",
    "    assert len(find) == 1\n",
    "    md5Hash = hashlib.md5(open(find[0], 'rb').read())\n",
    "    return md5Hash.hexdigest()\n",
    "\n",
    "\n",
    "def hash_fe(fe):\n",
    "    return hashlib.md5(','.join(sorted(fe.feature_names())).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def data_loader(task, df, queries, fe):\n",
    "    df_hash = hash_df(df)\n",
    "    jar_hash = hash_anserini_jar()\n",
    "    fe_hash = hash_fe(fe)\n",
    "    if os.path.exists(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle'):\n",
    "        res = pickle.load(open(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle','rb'))\n",
    "        print(res['data'].shape)\n",
    "        print(res['data'].qid.drop_duplicates().shape)\n",
    "        print(res['group'].mean())\n",
    "        print(res['data'].head(10))\n",
    "        print(res['data'].info())\n",
    "        return res\n",
    "    else:\n",
    "        if task == 'train' or task == 'dev': \n",
    "            data,group = extract(df, queries, fe)\n",
    "            obj = {'data':data,'group':group,'df_hash':df_hash,'jar_hash':jar_hash,'fe_hash':fe_hash}\n",
    "            print(data.shape)\n",
    "            print(data.qid.drop_duplicates().shape)\n",
    "            print(group.mean())\n",
    "            print(data.head(10))\n",
    "            print(data.info())\n",
    "            pickle.dump(obj,open(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle','wb'))\n",
    "            return obj\n",
    "        else:\n",
    "            raise Exception('unknown parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:12.992650Z",
     "start_time": "2020-12-10T19:04:12.984404Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def export(df, fn):\n",
    "    with open(fn,'w') as f:\n",
    "        for qid, group in tqdm(df.groupby('qid')):\n",
    "            line = {}\n",
    "            line['qid'] = qid\n",
    "            line['queryTokens'] = queries[qid]['tokenized']\n",
    "            line['queryText'] = queries[qid]['nonSW']\n",
    "            line['docIds'] = [str(did) for did in group.reset_index().pid.drop_duplicates().tolist()]\n",
    "            f.write(json.dumps(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:25.013754Z",
     "start_time": "2020-12-10T19:04:12.994294Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4416413, 76)\n",
      "(400782,)\n",
      "11.019489398226467\n",
      "   qid      pid  rel  contents_BM25_k1_0.90_b_0.40  \\\n",
      "0    3   970816    0                     22.893564   \n",
      "1    3  1142680    1                     26.467836   \n",
      "2    3  2019206    0                     19.257418   \n",
      "3    3  2605131    0                     13.279017   \n",
      "4    3  2963098    0                     19.064749   \n",
      "5    3  2971685    0                     13.103558   \n",
      "6    3  3783924    0                     18.477646   \n",
      "7    3  5067083    0                     13.021508   \n",
      "8    3  5904778    0                     12.939322   \n",
      "9    3  6176208    0                     18.572485   \n",
      "\n",
      "   contents_BM25_k1_1.20_b_0.75  contents_BM25_k1_2.00_b_0.75  \\\n",
      "0                     23.464577                     25.247410   \n",
      "1                     27.822981                     32.060062   \n",
      "2                     21.266153                     22.225826   \n",
      "3                     15.103254                     16.013908   \n",
      "4                     20.615219                     23.708914   \n",
      "5                     13.239493                     13.296593   \n",
      "6                     20.432346                     24.206491   \n",
      "7                     14.290549                     15.606924   \n",
      "8                     14.188528                     14.779155   \n",
      "9                     19.703806                     22.394455   \n",
      "\n",
      "   contents_LMD_mu_1000  contents_LMD_mu_1500  contents_LMD_mu_2500  \\\n",
      "0            -32.208611            -33.265926            -34.577877   \n",
      "1            -30.457207            -31.519070            -32.854885   \n",
      "2            -34.014656            -35.037079            -36.258217   \n",
      "3            -36.382740            -37.051022            -37.850716   \n",
      "4            -33.579044            -34.307404            -35.237865   \n",
      "5            -35.693199            -36.383183            -37.242317   \n",
      "6            -34.126472            -34.842415            -35.741966   \n",
      "7            -36.905354            -37.553295            -38.326641   \n",
      "8            -36.407188            -37.067444            -37.860626   \n",
      "9            -33.711987            -34.424625            -35.335533   \n",
      "\n",
      "   contents_LMJM_lambda_0.10  ...  contents_UnorderedSequentialPairs_15  \\\n",
      "0                 -27.848728  ...                                   4.0   \n",
      "1                 -26.552271  ...                                  11.0   \n",
      "2                 -27.976364  ...                                   2.0   \n",
      "3                 -35.830353  ...                                   0.0   \n",
      "4                 -34.013767  ...                                   8.0   \n",
      "5                 -36.210621  ...                                   1.0   \n",
      "6                 -34.347286  ...                                   0.0   \n",
      "7                 -36.938057  ...                                   0.0   \n",
      "8                 -36.276485  ...                                   0.0   \n",
      "9                 -34.424660  ...                                   2.0   \n",
      "\n",
      "   contents_OrderedSequentialPairs_3  contents_OrderedSequentialPairs_8  \\\n",
      "0                                2.0                                3.0   \n",
      "1                                3.0                                4.0   \n",
      "2                                2.0                                2.0   \n",
      "3                                0.0                                0.0   \n",
      "4                                2.0                                4.0   \n",
      "5                                1.0                                1.0   \n",
      "6                                0.0                                0.0   \n",
      "7                                0.0                                0.0   \n",
      "8                                0.0                                0.0   \n",
      "9                                0.0                                0.0   \n",
      "\n",
      "   contents_OrderedSequentialPairs_15  contents_UnorderedQueryPairs_3  \\\n",
      "0                                 3.0                             4.0   \n",
      "1                                 6.0                             4.0   \n",
      "2                                 2.0                             3.0   \n",
      "3                                 0.0                             1.0   \n",
      "4                                 7.0                             3.0   \n",
      "5                                 1.0                             1.0   \n",
      "6                                 0.0                             2.0   \n",
      "7                                 0.0                             0.0   \n",
      "8                                 0.0                             0.0   \n",
      "9                                 0.0                             0.0   \n",
      "\n",
      "   contents_UnorderedQueryPairs_8  contents_UnorderedQueryPairs_15  \\\n",
      "0                             6.0                              6.0   \n",
      "1                            10.0                             14.0   \n",
      "2                             3.0                              3.0   \n",
      "3                             1.0                              1.0   \n",
      "4                             5.0                              8.0   \n",
      "5                             1.0                              1.0   \n",
      "6                             5.0                              8.0   \n",
      "7                             0.0                              0.0   \n",
      "8                             0.0                              1.0   \n",
      "9                             2.0                              2.0   \n",
      "\n",
      "   contents_OrderedQueryPairs_3  contents_OrderedQueryPairs_8  \\\n",
      "0                           4.0                           5.0   \n",
      "1                           4.0                           6.0   \n",
      "2                           3.0                           3.0   \n",
      "3                           1.0                           1.0   \n",
      "4                           2.0                           4.0   \n",
      "5                           1.0                           1.0   \n",
      "6                           2.0                           5.0   \n",
      "7                           0.0                           0.0   \n",
      "8                           0.0                           0.0   \n",
      "9                           0.0                           0.0   \n",
      "\n",
      "   contents_OrderedQueryPairs_15  \n",
      "0                            5.0  \n",
      "1                            9.0  \n",
      "2                            3.0  \n",
      "3                            1.0  \n",
      "4                            7.0  \n",
      "5                            1.0  \n",
      "6                            5.0  \n",
      "7                            0.0  \n",
      "8                            0.0  \n",
      "9                            0.0  \n",
      "\n",
      "[10 rows x 76 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4416413 entries, 0 to 4416412\n",
      "Data columns (total 76 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   qid                                   int32  \n",
      " 1   pid                                   int32  \n",
      " 2   rel                                   int32  \n",
      " 3   contents_BM25_k1_0.90_b_0.40          float32\n",
      " 4   contents_BM25_k1_1.20_b_0.75          float32\n",
      " 5   contents_BM25_k1_2.00_b_0.75          float32\n",
      " 6   contents_LMD_mu_1000                  float32\n",
      " 7   contents_LMD_mu_1500                  float32\n",
      " 8   contents_LMD_mu_2500                  float32\n",
      " 9   contents_LMJM_lambda_0.10             float32\n",
      " 10  contents_LMJM_lambda_0.40             float32\n",
      " 11  contents_LMJM_lambda_0.70             float32\n",
      " 12  contents_NTFIDF                       float32\n",
      " 13  contents_Prob                         float32\n",
      " 14  contents_DFR_GL2                      float32\n",
      " 15  contents_DFR_In_expB2                 float32\n",
      " 16  contents_DPH                          float32\n",
      " 17  contents_Proximity                    float32\n",
      " 18  contents_TPscore                      float32\n",
      " 19  contents_tpDistWindow100              float32\n",
      " 20  contents_DocSize                      float32\n",
      " 21  Entropy                               float32\n",
      " 22  StopCover                             float32\n",
      " 23  StopRatio                             float32\n",
      " 24  QueryLength                           float32\n",
      " 25  QueryLengthNonStopWords               float32\n",
      " 26  contents_QueryCoverageRatio           float32\n",
      " 27  UniqueQueryTerms                      float32\n",
      " 28  contents_MatchingTermCount            float32\n",
      " 29  contents_SCS                          float32\n",
      " 30  contents_TF_avg                       float32\n",
      " 31  contents_TF_sum                       float32\n",
      " 32  contents_TF_min                       float32\n",
      " 33  contents_TF_max                       float32\n",
      " 34  contents_TF_var                       float32\n",
      " 35  contents_TFIDF_avg                    float32\n",
      " 36  contents_TFIDF_sum                    float32\n",
      " 37  contents_TFIDF_min                    float32\n",
      " 38  contents_TFIDF_max                    float32\n",
      " 39  contents_TFIDF_var                    float32\n",
      " 40  contents_SCQ_avg                      float32\n",
      " 41  contents_SCQ_sum                      float32\n",
      " 42  contents_SCQ_min                      float32\n",
      " 43  contents_SCQ_max                      float32\n",
      " 44  contents_SCQ_var                      float32\n",
      " 45  contents_NormalizedTF_avg             float32\n",
      " 46  contents_NormalizedTF_sum             float32\n",
      " 47  contents_NormalizedTF_min             float32\n",
      " 48  contents_NormalizedTF_max             float32\n",
      " 49  contents_NormalizedTF_var             float32\n",
      " 50  contents_IDF_avg                      float32\n",
      " 51  contents_IDF_sum                      float32\n",
      " 52  contents_IDF_min                      float32\n",
      " 53  contents_IDF_max                      float32\n",
      " 54  contents_IDF_var                      float32\n",
      " 55  contents_IDF_maxminratio              float32\n",
      " 56  contents_IDF_confidence               float32\n",
      " 57  contents_ICTF_avg                     float32\n",
      " 58  contents_ICTF_sum                     float32\n",
      " 59  contents_ICTF_min                     float32\n",
      " 60  contents_ICTF_max                     float32\n",
      " 61  contents_ICTF_var                     float32\n",
      " 62  contents_ICTF_maxminratio             float32\n",
      " 63  contents_ICTF_confidence              float32\n",
      " 64  contents_UnorderedSequentialPairs_3   float32\n",
      " 65  contents_UnorderedSequentialPairs_8   float32\n",
      " 66  contents_UnorderedSequentialPairs_15  float32\n",
      " 67  contents_OrderedSequentialPairs_3     float32\n",
      " 68  contents_OrderedSequentialPairs_8     float32\n",
      " 69  contents_OrderedSequentialPairs_15    float32\n",
      " 70  contents_UnorderedQueryPairs_3        float32\n",
      " 71  contents_UnorderedQueryPairs_8        float32\n",
      " 72  contents_UnorderedQueryPairs_15       float32\n",
      " 73  contents_OrderedQueryPairs_3          float32\n",
      " 74  contents_OrderedQueryPairs_8          float32\n",
      " 75  contents_OrderedQueryPairs_15         float32\n",
      "dtypes: float32(73), int32(3)\n",
      "memory usage: 1.3 GB\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6974598, 76)\n",
      "(6980,)\n",
      "999.2260744985673\n",
      "   qid     pid  rel  contents_BM25_k1_0.90_b_0.40  \\\n",
      "0    2   55860    0                     12.348820   \n",
      "1    2   72202    0                     10.927653   \n",
      "2    2   72210    0                     13.675473   \n",
      "3    2   98589    0                     12.699286   \n",
      "4    2   98590    0                     12.492470   \n",
      "5    2   98593    0                     11.077914   \n",
      "6    2   98595    0                     11.181725   \n",
      "7    2  112123    0                     15.955744   \n",
      "8    2  112126    0                     11.468307   \n",
      "9    2  112127    0                     21.200821   \n",
      "\n",
      "   contents_BM25_k1_1.20_b_0.75  contents_BM25_k1_2.00_b_0.75  \\\n",
      "0                     13.151269                     14.964364   \n",
      "1                     11.041017                     11.088634   \n",
      "2                     13.716912                     14.579938   \n",
      "3                     14.042357                     16.412817   \n",
      "4                     13.508439                     15.535631   \n",
      "5                     11.527296                     12.512784   \n",
      "6                     11.772854                     12.868898   \n",
      "7                     17.375216                     18.039532   \n",
      "8                     12.481902                     13.925775   \n",
      "9                     23.124138                     26.404234   \n",
      "\n",
      "   contents_LMD_mu_1000  contents_LMD_mu_1500  contents_LMD_mu_2500  \\\n",
      "0            -23.447887            -23.810581            -24.285263   \n",
      "1            -25.348633            -25.968309            -26.691536   \n",
      "2            -24.175011            -24.847307            -25.665592   \n",
      "3            -23.413172            -23.787159            -24.271072   \n",
      "4            -23.433472            -23.800844            -24.279358   \n",
      "5            -23.849216            -24.212214            -24.686415   \n",
      "6            -23.840567            -24.206371            -24.682871   \n",
      "7            -21.773283            -22.522568            -23.454348   \n",
      "8            -23.817379            -24.190737            -24.673403   \n",
      "9            -20.040480            -20.801128            -21.761345   \n",
      "\n",
      "   contents_LMJM_lambda_0.10  ...  contents_UnorderedSequentialPairs_15  \\\n",
      "0                 -24.888071  ...                                   0.0   \n",
      "1                 -21.576532  ...                                   1.0   \n",
      "2                 -20.818115  ...                                   2.0   \n",
      "3                 -24.560862  ...                                   0.0   \n",
      "4                 -24.764460  ...                                   0.0   \n",
      "5                 -25.270000  ...                                   0.0   \n",
      "6                 -25.195894  ...                                   0.0   \n",
      "7                 -16.985819  ...                                   1.0   \n",
      "8                 -24.966324  ...                                   0.0   \n",
      "9                 -15.670968  ...                                   6.0   \n",
      "\n",
      "   contents_OrderedSequentialPairs_3  contents_OrderedSequentialPairs_8  \\\n",
      "0                                0.0                                0.0   \n",
      "1                                0.0                                0.0   \n",
      "2                                0.0                                0.0   \n",
      "3                                0.0                                0.0   \n",
      "4                                0.0                                0.0   \n",
      "5                                0.0                                0.0   \n",
      "6                                0.0                                0.0   \n",
      "7                                1.0                                1.0   \n",
      "8                                0.0                                0.0   \n",
      "9                                2.0                                3.0   \n",
      "\n",
      "   contents_OrderedSequentialPairs_15  contents_UnorderedQueryPairs_3  \\\n",
      "0                                 0.0                             0.0   \n",
      "1                                 0.0                             0.0   \n",
      "2                                 0.0                             0.0   \n",
      "3                                 0.0                             0.0   \n",
      "4                                 0.0                             0.0   \n",
      "5                                 0.0                             0.0   \n",
      "6                                 0.0                             0.0   \n",
      "7                                 1.0                             1.0   \n",
      "8                                 0.0                             0.0   \n",
      "9                                 3.0                             3.0   \n",
      "\n",
      "   contents_UnorderedQueryPairs_8  contents_UnorderedQueryPairs_15  \\\n",
      "0                             0.0                              0.0   \n",
      "1                             0.0                              1.0   \n",
      "2                             0.0                              2.0   \n",
      "3                             0.0                              0.0   \n",
      "4                             0.0                              0.0   \n",
      "5                             0.0                              0.0   \n",
      "6                             0.0                              0.0   \n",
      "7                             1.0                              1.0   \n",
      "8                             0.0                              0.0   \n",
      "9                             4.0                              6.0   \n",
      "\n",
      "   contents_OrderedQueryPairs_3  contents_OrderedQueryPairs_8  \\\n",
      "0                           0.0                           0.0   \n",
      "1                           0.0                           0.0   \n",
      "2                           0.0                           0.0   \n",
      "3                           0.0                           0.0   \n",
      "4                           0.0                           0.0   \n",
      "5                           0.0                           0.0   \n",
      "6                           0.0                           0.0   \n",
      "7                           1.0                           1.0   \n",
      "8                           0.0                           0.0   \n",
      "9                           2.0                           3.0   \n",
      "\n",
      "   contents_OrderedQueryPairs_15  \n",
      "0                            0.0  \n",
      "1                            0.0  \n",
      "2                            0.0  \n",
      "3                            0.0  \n",
      "4                            0.0  \n",
      "5                            0.0  \n",
      "6                            0.0  \n",
      "7                            1.0  \n",
      "8                            0.0  \n",
      "9                            3.0  \n",
      "\n",
      "[10 rows x 76 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6974598 entries, 0 to 6974597\n",
      "Data columns (total 76 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   qid                                   int32  \n",
      " 1   pid                                   int32  \n",
      " 2   rel                                   int32  \n",
      " 3   contents_BM25_k1_0.90_b_0.40          float32\n",
      " 4   contents_BM25_k1_1.20_b_0.75          float32\n",
      " 5   contents_BM25_k1_2.00_b_0.75          float32\n",
      " 6   contents_LMD_mu_1000                  float32\n",
      " 7   contents_LMD_mu_1500                  float32\n",
      " 8   contents_LMD_mu_2500                  float32\n",
      " 9   contents_LMJM_lambda_0.10             float32\n",
      " 10  contents_LMJM_lambda_0.40             float32\n",
      " 11  contents_LMJM_lambda_0.70             float32\n",
      " 12  contents_NTFIDF                       float32\n",
      " 13  contents_Prob                         float32\n",
      " 14  contents_DFR_GL2                      float32\n",
      " 15  contents_DFR_In_expB2                 float32\n",
      " 16  contents_DPH                          float32\n",
      " 17  contents_Proximity                    float32\n",
      " 18  contents_TPscore                      float32\n",
      " 19  contents_tpDistWindow100              float32\n",
      " 20  contents_DocSize                      float32\n",
      " 21  Entropy                               float32\n",
      " 22  StopCover                             float32\n",
      " 23  StopRatio                             float32\n",
      " 24  QueryLength                           float32\n",
      " 25  QueryLengthNonStopWords               float32\n",
      " 26  contents_QueryCoverageRatio           float32\n",
      " 27  UniqueQueryTerms                      float32\n",
      " 28  contents_MatchingTermCount            float32\n",
      " 29  contents_SCS                          float32\n",
      " 30  contents_TF_avg                       float32\n",
      " 31  contents_TF_sum                       float32\n",
      " 32  contents_TF_min                       float32\n",
      " 33  contents_TF_max                       float32\n",
      " 34  contents_TF_var                       float32\n",
      " 35  contents_TFIDF_avg                    float32\n",
      " 36  contents_TFIDF_sum                    float32\n",
      " 37  contents_TFIDF_min                    float32\n",
      " 38  contents_TFIDF_max                    float32\n",
      " 39  contents_TFIDF_var                    float32\n",
      " 40  contents_SCQ_avg                      float32\n",
      " 41  contents_SCQ_sum                      float32\n",
      " 42  contents_SCQ_min                      float32\n",
      " 43  contents_SCQ_max                      float32\n",
      " 44  contents_SCQ_var                      float32\n",
      " 45  contents_NormalizedTF_avg             float32\n",
      " 46  contents_NormalizedTF_sum             float32\n",
      " 47  contents_NormalizedTF_min             float32\n",
      " 48  contents_NormalizedTF_max             float32\n",
      " 49  contents_NormalizedTF_var             float32\n",
      " 50  contents_IDF_avg                      float32\n",
      " 51  contents_IDF_sum                      float32\n",
      " 52  contents_IDF_min                      float32\n",
      " 53  contents_IDF_max                      float32\n",
      " 54  contents_IDF_var                      float32\n",
      " 55  contents_IDF_maxminratio              float32\n",
      " 56  contents_IDF_confidence               float32\n",
      " 57  contents_ICTF_avg                     float32\n",
      " 58  contents_ICTF_sum                     float32\n",
      " 59  contents_ICTF_min                     float32\n",
      " 60  contents_ICTF_max                     float32\n",
      " 61  contents_ICTF_var                     float32\n",
      " 62  contents_ICTF_maxminratio             float32\n",
      " 63  contents_ICTF_confidence              float32\n",
      " 64  contents_UnorderedSequentialPairs_3   float32\n",
      " 65  contents_UnorderedSequentialPairs_8   float32\n",
      " 66  contents_UnorderedSequentialPairs_15  float32\n",
      " 67  contents_OrderedSequentialPairs_3     float32\n",
      " 68  contents_OrderedSequentialPairs_8     float32\n",
      " 69  contents_OrderedSequentialPairs_15    float32\n",
      " 70  contents_UnorderedQueryPairs_3        float32\n",
      " 71  contents_UnorderedQueryPairs_8        float32\n",
      " 72  contents_UnorderedQueryPairs_15       float32\n",
      " 73  contents_OrderedQueryPairs_3          float32\n",
      " 74  contents_OrderedQueryPairs_8          float32\n",
      " 75  contents_OrderedQueryPairs_15         float32\n",
      "dtypes: float32(73), int32(3)\n",
      "memory usage: 2.0 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_extracted = data_loader('train', sampled_train, queries, fe)\n",
    "# export(sampled_train, 'sampled_train_export.json')\n",
    "dev_extracted = data_loader('dev', dev, queries, fe)\n",
    "# export(dev, 'sampled_dev_export.json')\n",
    "del sampled_train, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:25.020536Z",
     "start_time": "2020-12-10T19:04:25.015191Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_mrr(dev_data):\n",
    "    score_tie_counter = 0\n",
    "    score_tie_query = set()\n",
    "\n",
    "    MRR = []\n",
    "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
    "        group = group.reset_index()\n",
    "        rank = 0\n",
    "        prev_score = None\n",
    "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
    "        # stable sort is also used in LightGBM\n",
    "\n",
    "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
    "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
    "                score_tie_counter += 1\n",
    "                score_tie_query.add(qid)\n",
    "            prev_score = t.score\n",
    "            prev_pid = t.pid\n",
    "            rank += 1\n",
    "            if t.rel>0:\n",
    "                MRR.append(1.0/rank)\n",
    "                break\n",
    "            elif rank == 10 or rank == len(group):\n",
    "                MRR.append(0.)\n",
    "                break\n",
    "\n",
    "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
    "    print(score_tie,np.mean(MRR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:04:25.035169Z",
     "start_time": "2020-12-10T19:04:25.022252Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_recall(dev_qrel, dev_data):\n",
    "    dev_rel_num = dev_qrel[dev_qrel['rel']>0].groupby('qid').count()['rel']\n",
    "\n",
    "    score_tie_counter = 0\n",
    "    score_tie_query = set()\n",
    "    \n",
    "    recall_point = [10,20,50,100,200,500,1000]\n",
    "    recall_curve = {k:[] for k in recall_point}\n",
    "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
    "        group = group.reset_index()\n",
    "        rank = 0\n",
    "        prev_score = None\n",
    "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
    "        # stable sort is also used in LightGBM\n",
    "        total_rel = dev_rel_num.loc[qid]\n",
    "        query_recall = [0 for k in recall_point]\n",
    "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
    "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
    "                score_tie_counter += 1\n",
    "                score_tie_query.add(qid)\n",
    "            prev_score = t.score\n",
    "            rank += 1\n",
    "            if t.rel>0:\n",
    "                for i,p in enumerate(recall_point):\n",
    "                    if rank <= p:\n",
    "                        query_recall[i] += 1\n",
    "        for i,p in enumerate(recall_point):\n",
    "            if total_rel>0:\n",
    "                recall_curve[p].append(query_recall[i]/total_rel)\n",
    "            else:\n",
    "                recall_curve[p].append(0.)\n",
    "\n",
    "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
    "    print(score_tie)\n",
    "    \n",
    "    for k,v in recall_curve.items():\n",
    "        avg = np.mean(v)\n",
    "        print(f'recall@{k}:{avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T19:16:32.938092Z",
     "start_time": "2020-12-10T19:16:30.297266Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_name = fe.feature_names()\n",
    "train_X = train_extracted['data'].loc[:, feature_name]\n",
    "train_Y = train_extracted['data']['rel']\n",
    "dev_X = dev_extracted['data'].loc[:, feature_name]\n",
    "dev_Y = dev_extracted['data']['rel']\n",
    "lgb_train = lgb.Dataset(train_X,label=train_Y,group=train_extracted['group'])\n",
    "lgb_valid = lgb.Dataset(dev_X,label=dev_Y,group=dev_extracted['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T18:55:39.584750Z",
     "start_time": "2020-12-10T16:59:36.660936Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total groups: 320626, total data: 31790488\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790488, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80156, total data: 7947622\n",
      "[LightGBM] [Info] Total groups: 320625, total data: 31790487\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790487, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80157, total data: 7947623\n",
      "[LightGBM] [Info] Total groups: 320625, total data: 31790487\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790487, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80157, total data: 7947623\n",
      "[LightGBM] [Info] Total groups: 320626, total data: 31790489\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790489, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80156, total data: 7947621\n",
      "[LightGBM] [Info] Total groups: 320626, total data: 31790489\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790489, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80156, total data: 7947621\n",
      "[('contents_DFR_In_expB2', 2665), ('contents_SCQ_avg', 2270), ('contents_SCQ_var', 2180), ('contents_Prob', 2018), ('contents_SCQ_max', 1812), ('contents_NTFIDF', 1757), ('contents_NormalizedTF_var', 1684), ('contents_NormalizedTF_min', 1635), ('contents_TFIDF_var', 1599), ('contents_DocSize', 1584), ('contents_TFIDF_max', 1576), ('contents_DFR_GL2', 1508), ('contents_NormalizedTF_avg', 1484), ('contents_Proximity', 1458), ('contents_IDF_maxminratio', 1438), ('contents_TPscore', 1394), ('contents_ICTF_maxminratio', 1342), ('contents_TF_var', 1304), ('contents_NormalizedTF_sum', 1223), ('contents_TFIDF_avg', 1199), ('contents_BM25_k1_2.00_b_0.75', 1190), ('contents_IDF_sum', 1139), ('contents_TFIDF_sum', 1089), ('contents_BM25_k1_0.90_b_0.40', 970), ('contents_LMJM_lambda_0.70', 958), ('contents_IDF_avg', 956), ('contents_SCQ_sum', 935), ('contents_LMJM_lambda_0.10', 934), ('contents_SCS', 904), ('contents_ICTF_max', 890), ('contents_ICTF_confidence', 829), ('contents_UnorderedQueryPairs_15', 826), ('contents_IDF_max', 824), ('contents_BM25_k1_1.20_b_0.75', 811), ('contents_ICTF_avg', 798), ('contents_LMD_mu_1000', 787), ('contents_TF_avg', 781), ('contents_IDF_confidence', 775), ('contents_UnorderedSequentialPairs_15', 775), ('contents_LMJM_lambda_0.40', 773), ('contents_ICTF_var', 743), ('contents_IDF_var', 720), ('contents_UnorderedQueryPairs_8', 646), ('contents_QueryCoverageRatio', 636), ('contents_UnorderedQueryPairs_3', 628), ('contents_ICTF_sum', 602), ('QueryLength', 601), ('contents_OrderedQueryPairs_8', 601), ('contents_OrderedQueryPairs_3', 577), ('contents_OrderedQueryPairs_15', 532), ('contents_OrderedSequentialPairs_3', 526), ('contents_UnorderedSequentialPairs_8', 484), ('contents_LMD_mu_2500', 481), ('contents_TF_sum', 453), ('contents_UnorderedSequentialPairs_3', 450), ('contents_OrderedSequentialPairs_15', 428), ('contents_LMD_mu_1500', 397), ('contents_OrderedSequentialPairs_8', 389), ('contents_MatchingTermCount', 358), ('contents_TF_max', 340), ('contents_tpDistWindow100', 197), ('UniqueQueryTerms', 63), ('QueryLengthNonStopWords', 38), ('StopRatio', 34), ('contents_DPH', 2), ('Entropy', 0), ('StopCover', 0), ('contents_TF_min', 0), ('contents_TFIDF_min', 0), ('contents_SCQ_min', 0), ('contents_NormalizedTF_max', 0), ('contents_IDF_min', 0), ('contents_ICTF_min', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [01:43<00:00, 67.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 1653353 times in 6978 queries\n",
      "recall@10:0.4295367717287488\n",
      "recall@20:0.5235912129894937\n",
      "recall@50:0.6277578796561605\n",
      "recall@100:0.7038443170964662\n",
      "recall@200:0.7657712511938873\n",
      "recall@500:0.8312082139446035\n",
      "recall@1000:0.8573424068767909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:57<00:00, 122.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 934 times in 682 queries 0.2141419929958612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'max_bin':255,\n",
    "    'num_leaves':63,\n",
    "    'max_depth':-1,\n",
    "    'min_data_in_leaf':50,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':50,\n",
    "    'feature_fraction':1,\n",
    "    'learning_rate':0.1,\n",
    "    'num_boost_round':1000,\n",
    "    'metric':['map'],\n",
    "    'eval_at':[10],\n",
    "    'label_gain':[0,1],\n",
    "    'lambdarank_truncation_level':20,\n",
    "    'force_col_wise':True,\n",
    "    'seed':12345,\n",
    "    'num_threads':max(multiprocessing.cpu_count()//2,1)\n",
    "}\n",
    "\n",
    "num_boost_round = params.pop('num_boost_round')\n",
    "eval_results={}\n",
    "cv_gbm = lgb.cv(params, lgb_train, nfold=5, \n",
    "                num_boost_round=num_boost_round,\n",
    "                feature_name=feature_name,\n",
    "                verbose_eval=False,\n",
    "                return_cvbooster=True)\n",
    "dev_extracted['data']['score'] = 0.\n",
    "for gbm in cv_gbm['cvbooster'].boosters:\n",
    "    dev_extracted['data']['score']+=gbm.predict(dev_X)\n",
    "feature_importances = sorted(list(zip(feature_name,gbm.feature_importance().tolist())),key=lambda x:x[1],reverse=True)\n",
    "print(feature_importances)\n",
    "eval_recall(dev_qrel, dev_extracted['data'])\n",
    "eval_mrr(dev_extracted['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total groups: 320626, total data: 31790488\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790488, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80156, total data: 7947622\n",
      "[LightGBM] [Info] Total groups: 320625, total data: 31790487\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790487, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80157, total data: 7947623\n",
      "[LightGBM] [Info] Total groups: 320625, total data: 31790487\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790487, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80157, total data: 7947623\n",
      "[LightGBM] [Info] Total groups: 320626, total data: 31790489\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790489, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80156, total data: 7947621\n",
      "[LightGBM] [Info] Total groups: 320626, total data: 31790489\n",
      "[LightGBM] [Info] Total Bins 11913\n",
      "[LightGBM] [Info] Number of data points in the train set: 31790489, number of used features: 66\n",
      "[LightGBM] [Info] Total groups: 80156, total data: 7947621\n",
      "[('contents_DFR_In_expB2', 2713), ('contents_SCQ_avg', 2316), ('contents_SCQ_var', 2249), ('contents_Prob', 2023), ('contents_SCQ_max', 1737), ('contents_NTFIDF', 1721), ('contents_NormalizedTF_var', 1719), ('contents_TFIDF_var', 1670), ('contents_NormalizedTF_min', 1632), ('contents_DocSize', 1576), ('contents_NormalizedTF_avg', 1534), ('contents_DFR_GL2', 1530), ('contents_IDF_maxminratio', 1485), ('contents_TFIDF_max', 1479), ('contents_Proximity', 1449), ('contents_ICTF_maxminratio', 1347), ('contents_TPscore', 1313), ('contents_TF_var', 1279), ('contents_IDF_sum', 1236), ('contents_NormalizedTF_sum', 1145), ('contents_BM25_k1_2.00_b_0.75', 1125), ('contents_TFIDF_avg', 1124), ('contents_TFIDF_sum', 1101), ('contents_LMJM_lambda_0.70', 1039), ('contents_SCS', 994), ('contents_BM25_k1_0.90_b_0.40', 987), ('contents_LMJM_lambda_0.10', 977), ('contents_SCQ_sum', 921), ('contents_IDF_avg', 885), ('contents_UnorderedQueryPairs_15', 869), ('contents_UnorderedSequentialPairs_15', 844), ('contents_ICTF_avg', 824), ('contents_ICTF_max', 793), ('contents_LMD_mu_1000', 783), ('contents_BM25_k1_1.20_b_0.75', 780), ('contents_TF_avg', 771), ('contents_IDF_confidence', 768), ('contents_ICTF_confidence', 754), ('contents_LMJM_lambda_0.40', 748), ('contents_IDF_max', 724), ('contents_IDF_var', 702), ('contents_ICTF_var', 690), ('contents_UnorderedQueryPairs_8', 643), ('QueryLength', 641), ('contents_QueryCoverageRatio', 630), ('contents_ICTF_sum', 624), ('contents_OrderedQueryPairs_8', 624), ('contents_OrderedQueryPairs_3', 593), ('contents_UnorderedQueryPairs_3', 588), ('contents_OrderedQueryPairs_15', 553), ('contents_OrderedSequentialPairs_3', 537), ('contents_LMD_mu_2500', 524), ('contents_UnorderedSequentialPairs_3', 486), ('contents_UnorderedSequentialPairs_8', 448), ('contents_TF_sum', 440), ('contents_LMD_mu_1500', 433), ('contents_OrderedSequentialPairs_15', 420), ('contents_OrderedSequentialPairs_8', 400), ('contents_MatchingTermCount', 399), ('contents_TF_max', 318), ('contents_tpDistWindow100', 166), ('UniqueQueryTerms', 70), ('QueryLengthNonStopWords', 63), ('StopRatio', 39), ('contents_DPH', 5), ('Entropy', 0), ('StopCover', 0), ('contents_TF_min', 0), ('contents_TFIDF_min', 0), ('contents_SCQ_min', 0), ('contents_NormalizedTF_max', 0), ('contents_IDF_min', 0), ('contents_ICTF_min', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [01:41<00:00, 69.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 1650194 times in 6978 queries\n",
      "recall@10:0.42884431709646603\n",
      "recall@20:0.5267669531996179\n",
      "recall@50:0.6276026743075452\n",
      "recall@100:0.7023280802292263\n",
      "recall@200:0.7663204393505254\n",
      "recall@500:0.8302531041069722\n",
      "recall@1000:0.8573424068767909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:47<00:00, 145.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 910 times in 670 queries 0.2147141492700232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'max_bin':255,\n",
    "    'num_leaves':63,\n",
    "    'max_depth':10,\n",
    "    'min_data_in_leaf':50,\n",
    "    'min_sum_hessian_in_leaf':0,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':50,\n",
    "    'feature_fraction':1,\n",
    "    'learning_rate':0.1,\n",
    "    'num_boost_round':1000,\n",
    "    'metric':['map'],\n",
    "    'eval_at':[10],\n",
    "    'label_gain':[0,1],\n",
    "    'lambdarank_truncation_level':20,\n",
    "    'force_col_wise':True,\n",
    "    'seed':12345,\n",
    "    'num_threads':max(multiprocessing.cpu_count()//2,1)\n",
    "}\n",
    "\n",
    "num_boost_round = params.pop('num_boost_round')\n",
    "eval_results={}\n",
    "cv_gbm = lgb.cv(params, lgb_train, nfold=5, \n",
    "                num_boost_round=num_boost_round,\n",
    "                feature_name=feature_name,\n",
    "                verbose_eval=False,\n",
    "                return_cvbooster=True)\n",
    "dev_extracted['data']['score'] = 0.\n",
    "for gbm in cv_gbm['cvbooster'].boosters:\n",
    "    dev_extracted['data']['score']+=gbm.predict(dev_X)\n",
    "feature_importances = sorted(list(zip(feature_name,gbm.feature_importance().tolist())),key=lambda x:x[1],reverse=True)\n",
    "print(feature_importances)\n",
    "eval_recall(dev_qrel, dev_extracted['data'])\n",
    "eval_mrr(dev_extracted['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
