{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51893214it [02:35, 333813.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# read T5 scores for each seg\n",
    "import collections\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "scores = collections.defaultdict(dict)\n",
    "# Merge all passage scores into a list of scores\n",
    "with open('./d2qseg.dev.tsv') as f_query_run_ids, open(\n",
    "        './mono_preds_d2qseg.dev.t5') as f_pred:\n",
    "    for line_query_passage_id, line_pred in tqdm(zip(f_query_run_ids, f_pred)):\n",
    "        query_id, passage_id, _ = line_query_passage_id.strip().split('\\t')\n",
    "        docseg_id = passage_id.split('.')[0]\n",
    "        _, score = line_pred.strip().split('\\t')\n",
    "        score = float(score)\n",
    "        scores[query_id][docseg_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5191985it [00:13, 394191.20it/s]\n",
      "100%|██████████| 5193/5193 [00:05<00:00, 865.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#get T5 for each seg for bm25+d2q top1000 and take maxP for each doc and reranked \n",
    "examples = collections.defaultdict(dict)\n",
    "with open('./run.doc.dev.1000.small.tsv') as fcan, open('./run.monoT5.doc.1000.trec', 'w') as fout:\n",
    "    for line_query_passage_id in tqdm(fcan):\n",
    "        query_id, passage_id, _ = line_query_passage_id.strip().split('\\t')\n",
    "        docseg_id = passage_id.split('.')[0]\n",
    "        doc_id = docseg_id.split('#')[0]\n",
    "        score = scores[query_id][docseg_id]\n",
    "        score = float(score)\n",
    "        if doc_id not in examples[query_id]:\n",
    "            examples[query_id][doc_id] = [score]\n",
    "        else:\n",
    "            examples[query_id][doc_id].append(score)\n",
    "    \n",
    "    for query_id, doc_ids_scores in tqdm(examples.items()):\n",
    "        doc_ids_scores = [\n",
    "            (doc_id, max(scores))\n",
    "            for doc_id, scores in doc_ids_scores.items()]\n",
    "\n",
    "        doc_ids_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        rank = 1\n",
    "        for doc_id, score in doc_ids_scores:\n",
    "            score = math.exp(score)\n",
    "            # Adds small score based on rank to avoid duplicated scores.\n",
    "            score += 1e-6 / rank\n",
    "            fout.write(f'{query_id} Q0 {doc_id} {rank} {score} T5\\n')\n",
    "            rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5193/5193 [00:02<00:00, 2455.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#only takes top 100 for each doc for msmarco_doc_eval.py\n",
    "with open ('./run.monoT5.doc.1000.tsv', 'w') as fout:\n",
    "    for query_id, doc_ids_scores in tqdm(examples.items()):\n",
    "            doc_ids_scores = [\n",
    "                (doc_id, max(scores))\n",
    "                for doc_id, scores in doc_ids_scores.items()]\n",
    "\n",
    "            doc_ids_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            rank = 1\n",
    "            for doc_id, score in doc_ids_scores:\n",
    "                if (rank < 100):\n",
    "                    score = math.exp(score)\n",
    "            # Adds small score based on rank to avoid duplicated scores.\n",
    "                    score += 1e-6 / rank\n",
    "                    fout.write(f'{query_id}\\t{doc_id}\\t{rank}\\n')\n",
    "                rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of Documents ranked for each query is as expected. Evaluating\n",
      "#####################\n",
      "MRR @100: 0.40878809930075677\n",
      "QueriesRanked: 5193\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "!python3 ../RA/pyserini/tools/scripts/msmarco/msmarco_doc_eval.py \\\n",
    " --judgments ../RA/pyserini/tools/topics-and-qrels/qrels.msmarco-doc.dev.txt --run ./run.monoT5.doc.1000.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ../RA/pyserini/tools/scripts/msmarco/convert_msmarco_to_trec_run.py --input ./run.monoT5.doc.1000.tsv --output ./run.monoT5.doc.1000.trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                   \tall\t0.4088\r\n",
      "ndcg_cut_10           \tall\t0.4760\r\n"
     ]
    }
   ],
   "source": [
    "!../RA/pyserini/tools/eval/trec_eval.9.0.4/trec_eval -c -mmap -m ndcg_cut.10\\\n",
    " ../RA/pyserini/tools/topics-and-qrels/qrels.msmarco-doc.dev.txt ./run.monoT5.doc.1000.trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5191985it [00:05, 881457.24it/s] \n",
      "5191985it [00:06, 824838.94it/s]\n",
      "5191985it [00:10, 515200.70it/s]\n",
      "5191985it [00:11, 456653.82it/s]\n",
      "5191985it [00:11, 440796.72it/s]\n",
      "5191985it [00:13, 397367.77it/s]\n",
      "5191985it [00:12, 399907.77it/s]\n",
      "5191985it [00:10, 510947.22it/s]\n",
      "5191985it [00:12, 400384.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for ltr_k in [100,200,300,400,500,600,700,800,900]:\n",
    "    for k in [1000]:\n",
    "        counts = None\n",
    "        counts = collections.defaultdict(int)\n",
    "        with open(f'run.doc.dev.{ltr_k}from{k}.tsv', 'w') as output_f:\n",
    "            with open(f'run.doc_pas.dev.d2q.1000.small.tsv') as input_f:\n",
    "                for line in tqdm(input_f):\n",
    "                    cols = line.split('\\t')\n",
    "                    qid = cols[0]\n",
    "                    if(counts[qid] < ltr_k):\n",
    "                        docid = cols[1]\n",
    "                        counts[qid] += 1\n",
    "                        output_f.write(f'{qid}\\t{docid}\\t{counts[qid]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "519300it [00:08, 60063.90it/s]\n",
      "100%|██████████| 5193/5193 [00:00<00:00, 11534.36it/s]\n",
      "1038600it [00:04, 259580.31it/s]\n",
      "100%|██████████| 5193/5193 [00:00<00:00, 5824.26it/s]\n",
      "1557900it [00:06, 248778.58it/s]\n",
      "100%|██████████| 5193/5193 [00:01<00:00, 3485.43it/s]\n",
      "2077126it [00:08, 235358.17it/s]\n",
      "100%|██████████| 5193/5193 [00:01<00:00, 2807.06it/s]\n",
      "2596326it [00:10, 255509.14it/s]\n",
      "100%|██████████| 5193/5193 [00:01<00:00, 2622.48it/s]\n",
      "3115526it [00:11, 280354.18it/s]\n",
      "100%|██████████| 5193/5193 [00:02<00:00, 2043.94it/s]\n",
      "3634685it [00:12, 291312.89it/s]\n",
      "100%|██████████| 5193/5193 [00:02<00:00, 2262.26it/s]\n",
      "4153785it [00:15, 264876.21it/s]\n",
      "100%|██████████| 5193/5193 [00:02<00:00, 1962.38it/s]\n",
      "4672885it [00:18, 254447.30it/s]\n",
      "100%|██████████| 5193/5193 [00:03<00:00, 1468.33it/s]\n"
     ]
    }
   ],
   "source": [
    "#get T5 for each seg for bm25+d2q top1000 and take maxP for each doc and reranked \n",
    "\n",
    "for ltr_k in [100,200,300,400,500,600,700,800,900]:\n",
    "    for k in [1000]:\n",
    "        examples = None\n",
    "        examples = collections.defaultdict(dict)\n",
    "        with open(f'./run.doc.dev.{ltr_k}from{k}.tsv') as fcan, open(f'./run.monoT5.doc.{ltr_k}from1000.d2q_baseline.tsv', 'w') as fout:\n",
    "            for line_query_passage_id in tqdm(fcan):\n",
    "                query_id, passage_id, _ = line_query_passage_id.strip().split('\\t')\n",
    "                docseg_id = passage_id.split('.')[0]\n",
    "                doc_id = docseg_id.split('#')[0]\n",
    "                if docseg_id in scores[query_id]:\n",
    "                    score = scores[query_id][docseg_id]\n",
    "                    score = float(score)\n",
    "                    if doc_id not in examples[query_id]:\n",
    "                        examples[query_id][doc_id] = [score]\n",
    "                    else:\n",
    "                        examples[query_id][doc_id].append(score)\n",
    "    \n",
    "            for query_id, doc_ids_scores in tqdm(examples.items()):\n",
    "                doc_ids_scores = [\n",
    "                    (doc_id, max(scores))\n",
    "                    for doc_id, scores in doc_ids_scores.items()]\n",
    "\n",
    "                doc_ids_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                rank = 1\n",
    "                for doc_id, score in doc_ids_scores:\n",
    "                    score = math.exp(score)\n",
    "                    # Adds small score based on rank to avoid duplicated scores.\n",
    "                    score += 1e-6 / rank\n",
    "                    fout.write(f'{query_id}\\t{doc_id}\\t{rank}\\n')\n",
    "                    rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.399996988532227\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "200 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.4050033080597611\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "300 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.40691860737864943\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "400 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.4074320235975577\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "500 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.4085256650058111\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "600 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.4087358364737934\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "700 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.4092244804119087\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "800 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.40896091715694155\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n",
      "900 from 1000:\n",
      " Evaluating\n",
      "#####################\n",
      "MRR @100: 0.4088245069595744\n",
      "QueriesRanked: 5193\n",
      "#####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "for ltr_k in [100,200,300,400,500,600,700,800,900]:\n",
    "    for k in [1000]:\n",
    "        stream = os.popen(f\"python3 ../RA/pyserini/tools/scripts/msmarco/msmarco_doc_eval.py \\\n",
    "         --judgments ../RA/pyserini/tools/topics-and-qrels/qrels.msmarco-doc.dev.txt --run run.monoT5.doc.{ltr_k}from1000.d2q_baseline.tsv\")\n",
    "        output = stream.read()\n",
    "        outputs = output.split('Quantity of Documents ranked for each query is as expected.')\n",
    "        print(f'{ltr_k} from {k}:')\n",
    "        print(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
