{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:35:07.374713Z",
     "start_time": "2020-12-24T05:34:58.212906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4416413, 1)\n",
      "(400782,)\n",
      "rel    11.019489\n",
      "dtype: float64\n",
      "             rel\n",
      "qid pid         \n",
      "3   970816     0\n",
      "    1142680    1\n",
      "    2019206    0\n",
      "    2605131    0\n",
      "    2963098    0\n",
      "    2971685    0\n",
      "    3783924    0\n",
      "    5067083    0\n",
      "    5904778    0\n",
      "    6176208    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4416413 entries, (3, 970816) to (1185869, 7770561)\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   rel     int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 166.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import json\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from collections import defaultdict\n",
    "from lightgbm.sklearn import LGBMRanker\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "from pyserini.ltr import *\n",
    "from pyserini.search import get_topics_with_reader\n",
    "\n",
    "def train_data_loader(task='triple', neg_sample=10, cutoff=100, random_seed=12345):\n",
    "    if task == 'triple' or task == 'rank':\n",
    "        fn = f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle'\n",
    "    elif task == 'rank_recall' or task == 'rank_hard':\n",
    "        fn = f'train_{task}_sampled_with_{neg_sample}_{cutoff}_{random_seed}.pickle'\n",
    "    else:\n",
    "        raise Exception('unknown parameters')\n",
    "    if os.path.exists(fn):\n",
    "        sampled_train = pd.read_pickle(fn)\n",
    "        print(sampled_train.shape)\n",
    "        print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(sampled_train.groupby('qid').count().mean())\n",
    "        print(sampled_train.head(10))\n",
    "        print(sampled_train.info())\n",
    "        return sampled_train\n",
    "    else:\n",
    "        if task == 'triple':\n",
    "            train = pd.read_csv('collections/msmarco-passage/qidpidtriples.train.full.2.tsv', sep=\"\\t\",\n",
    "                                names=['qid', 'pos_pid', 'neg_pid'], dtype=np.int32)\n",
    "            pos_half = train[['qid', 'pos_pid']].rename(columns={\"pos_pid\": \"pid\"}).drop_duplicates()\n",
    "            pos_half['rel'] = np.int32(1)\n",
    "            neg_half = train[['qid', 'neg_pid']].rename(columns={\"neg_pid\": \"pid\"}).drop_duplicates()\n",
    "            neg_half['rel'] = np.int32(0)\n",
    "            del train\n",
    "            sampled_neg_half = []\n",
    "            for qid, group in tqdm(neg_half.groupby('qid')):\n",
    "                sampled_neg_half.append(group.sample(n=min(neg_sample, len(group)), random_state=random_seed))\n",
    "            sampled_train = pd.concat([pos_half] + sampled_neg_half, axis=0, ignore_index=True)\n",
    "            sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "            print(sampled_train.shape)\n",
    "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "            print(sampled_train.groupby('qid').count().mean())\n",
    "            print(sampled_train.head(10))\n",
    "            print(sampled_train.info())\n",
    "\n",
    "            sampled_train.to_pickle(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        elif task == 'rank':\n",
    "            qrel = defaultdict(list)\n",
    "            with open(\"collections/msmarco-passage/qrels.train.tsv\") as f:\n",
    "                for line in f:\n",
    "                    topicid, _, docid, rel = line.strip().split('\\t')\n",
    "                    assert rel == \"1\", line.split(' ')\n",
    "                    qrel[topicid].append(docid)\n",
    "            \n",
    "            qid2pos = defaultdict(list)\n",
    "            qid2neg = defaultdict(list)\n",
    "            with open(\"runs/msmarco-passage/run.train.small.tsv\") as f:\n",
    "                for line in tqdm(f):\n",
    "                    topicid, docid, rank = line.split()\n",
    "                    assert topicid in qrel\n",
    "                    if docid in qrel[topicid]:\n",
    "                        qid2pos[topicid].append(docid)\n",
    "                    else:\n",
    "                        qid2neg[topicid].append(docid)\n",
    "            sampled_train = []\n",
    "            for topicid, pos_list in tqdm(qid2pos.items()):\n",
    "                neg_list = random.sample(qid2neg[topicid], min(len(qid2neg[topicid]), neg_sample))\n",
    "                for positive_docid in pos_list:\n",
    "                    sampled_train.append((int(topicid), int(positive_docid), 1))\n",
    "                for negative_docid in neg_list:\n",
    "                    sampled_train.append((int(topicid), int(negative_docid), 0))\n",
    "            sampled_train = pd.DataFrame(sampled_train,columns=['qid','pid','rel'],dtype=np.int32)\n",
    "            sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "            print(sampled_train.shape)\n",
    "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "            print(sampled_train.groupby('qid').count().mean())\n",
    "            print(sampled_train.head(10))\n",
    "            print(sampled_train.info())\n",
    "\n",
    "            sampled_train.to_pickle(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
    "        elif task == 'rank_recall':\n",
    "            qrel = defaultdict(list)\n",
    "            with open(\"../collections/msmarco-passage/qrels.train.tsv\") as f:\n",
    "                for line in f:\n",
    "                    topicid, _, docid, rel = line.strip().split('\\t')\n",
    "                    assert rel == \"1\", line.split(' ')\n",
    "                    qrel[topicid].append(docid)\n",
    "            \n",
    "            qid2pos = defaultdict(list)\n",
    "            qid2neg = defaultdict(list)\n",
    "            with open(\"../runs/msmarco-passage/run.train.small.tsv\") as f:\n",
    "                for line in tqdm(f):\n",
    "                    topicid, docid, rank = line.split()\n",
    "                    assert topicid in qrel\n",
    "                    if docid in qrel[topicid]:\n",
    "                        qid2pos[topicid].append(docid)\n",
    "                    else:\n",
    "                        if int(rank) > cutoff:\n",
    "                            qid2neg[topicid].append(docid)\n",
    "            sampled_train = []\n",
    "            for topicid, pos_list in tqdm(qid2pos.items()):\n",
    "                neg_list = random.sample(qid2neg[topicid], min(len(qid2neg[topicid]), neg_sample))\n",
    "                for positive_docid in pos_list:\n",
    "                    sampled_train.append((int(topicid), int(positive_docid), 1))\n",
    "                for negative_docid in neg_list:\n",
    "                    sampled_train.append((int(topicid), int(negative_docid), 0))\n",
    "            sampled_train = pd.DataFrame(sampled_train,columns=['qid','pid','rel'],dtype=np.int32)\n",
    "            sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "            print(sampled_train.shape)\n",
    "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "            print(sampled_train.groupby('qid').count().mean())\n",
    "            print(sampled_train.head(10))\n",
    "            print(sampled_train.info())\n",
    "\n",
    "            sampled_train.to_pickle(f'train_{task}_sampled_with_{neg_sample}_{cutoff}_{random_seed}.pickle')\n",
    "        elif task == 'rank_hard':\n",
    "            qrel = defaultdict(list)\n",
    "            with open(\"../collections/msmarco-passage/qrels.train.tsv\") as f:\n",
    "                for line in f:\n",
    "                    topicid, _, docid, rel = line.strip().split('\\t')\n",
    "                    assert rel == \"1\", line.split(' ')\n",
    "                    qrel[topicid].append(docid)\n",
    "            \n",
    "            qid2pos = defaultdict(list)\n",
    "            qid2neg = defaultdict(list)\n",
    "            with open(\"../runs/msmarco-passage/run.train.small.tsv\") as f:\n",
    "                for line in tqdm(f):\n",
    "                    topicid, docid, rank = line.split()\n",
    "                    assert topicid in qrel\n",
    "                    if docid in qrel[topicid]:\n",
    "                        qid2pos[topicid].append(docid)\n",
    "                    else:\n",
    "                        if int(rank) < cutoff:\n",
    "                            qid2neg[topicid].append(docid)\n",
    "            sampled_train = []\n",
    "            for topicid, pos_list in tqdm(qid2pos.items()):\n",
    "                neg_list = random.sample(qid2neg[topicid], min(len(qid2neg[topicid]), neg_sample))\n",
    "                for positive_docid in pos_list:\n",
    "                    sampled_train.append((int(topicid), int(positive_docid), 1))\n",
    "                for negative_docid in neg_list:\n",
    "                    sampled_train.append((int(topicid), int(negative_docid), 0))\n",
    "            sampled_train = pd.DataFrame(sampled_train,columns=['qid','pid','rel'],dtype=np.int32)\n",
    "            sampled_train = sampled_train.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "            print(sampled_train.shape)\n",
    "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
    "            print(sampled_train.groupby('qid').count().mean())\n",
    "            print(sampled_train.head(10))\n",
    "            print(sampled_train.info())\n",
    "\n",
    "            sampled_train.to_pickle(f'train_{task}_sampled_with_{neg_sample}_{cutoff}_{random_seed}.pickle')\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "        return sampled_train\n",
    "sampled_train = train_data_loader(task='triple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:35:08.823553Z",
     "start_time": "2020-12-24T05:35:07.376995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6974598, 2)\n",
      "(6980,)\n",
      "rank    999.226074\n",
      "rel     999.226074\n",
      "dtype: float64\n",
      "            rank  rel\n",
      "qid pid              \n",
      "2   55860    345    0\n",
      "    72202    557    0\n",
      "    72210    213    0\n",
      "    98589    278    0\n",
      "    98590    323    0\n",
      "    98593    580    0\n",
      "    98595    553    0\n",
      "    112123   108    0\n",
      "    112126   469    0\n",
      "    112127    21    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 6974598 entries, (2, 55860) to (1102400, 8830447)\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype\n",
      "---  ------  -----\n",
      " 0   rank    int32\n",
      " 1   rel     int32\n",
      "dtypes: int32(2)\n",
      "memory usage: 282.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def dev_data_loader(task='pygaggle'):\n",
    "    if os.path.exists(f'dev_{task}.pickle'):\n",
    "        dev = pd.read_pickle(f'dev_{task}.pickle')\n",
    "        print(dev.shape)\n",
    "        print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(dev.groupby('qid').count().mean())\n",
    "        print(dev.head(10))\n",
    "        print(dev.info())\n",
    "        dev_qrel = pd.read_pickle(f'dev_qrel.pickle')\n",
    "        return dev, dev_qrel\n",
    "    else:\n",
    "        if task == 'rerank':\n",
    "            dev = pd.read_csv('collections/msmarco-passage/top1000.dev', sep=\"\\t\",\n",
    "                              names=['qid', 'pid', 'query', 'doc'], usecols=['qid', 'pid'], dtype=np.int32)\n",
    "        elif task == 'anserini':\n",
    "            dev = pd.read_csv('runs/msmarco-passage/run.msmarco-passage.dev.small.tsv',sep=\"\\t\",\n",
    "                            names=['qid','pid','rank'], dtype=np.int32)\n",
    "        elif task == 'pygaggle':\n",
    "            dev = pd.read_csv('../pygaggle/data/msmarco_ans_entire/run.dev.small.tsv',sep=\"\\t\",\n",
    "                            names=['qid','pid','rank'], dtype=np.int32)\n",
    "        else:\n",
    "            raise Exception('unknown parameters')\n",
    "        dev_qrel = pd.read_csv('collections/msmarco-passage/qrels.dev.small.tsv', sep=\"\\t\",\n",
    "                               names=[\"qid\", \"q0\", \"pid\", \"rel\"], usecols=['qid', 'pid', 'rel'], dtype=np.int32)\n",
    "        dev = dev.merge(dev_qrel, left_on=['qid', 'pid'], right_on=['qid', 'pid'], how='left')\n",
    "        dev['rel'] = dev['rel'].fillna(0).astype(np.int32)\n",
    "        dev = dev.sort_values(['qid','pid']).set_index(['qid','pid'])\n",
    "        \n",
    "        print(dev.shape)\n",
    "        print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
    "        print(dev.groupby('qid').count().mean())\n",
    "        print(dev.head(10))\n",
    "        print(dev.info())\n",
    "\n",
    "        dev.to_pickle(f'dev_{task}.pickle')\n",
    "        dev_qrel.to_pickle(f'dev_qrel.pickle')\n",
    "        return dev, dev_qrel\n",
    "dev, dev_qrel = dev_data_loader(task='pygaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:35:18.233519Z",
     "start_time": "2020-12-24T05:35:08.828509Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_loader():\n",
    "    queries = {}\n",
    "    with open('queries.train.small.Flex.json') as f:\n",
    "        for line in f:\n",
    "            query = json.loads(line)\n",
    "            qid = query.pop('id')\n",
    "            query['analyzed'] = query['analyzed'].split(\" \")\n",
    "            query['text'] = query['text'].split(\" \")\n",
    "            query['text_unlemm'] = query['text_unlemm'].split(\" \")\n",
    "            query['text_bert_tok'] = query['text_bert_tok'].split(\" \")\n",
    "            queries[qid] = query\n",
    "    with open('queries.dev.small.Flex.json') as f:\n",
    "        for line in f:\n",
    "            query = json.loads(line)\n",
    "            qid = query.pop('id')\n",
    "            query['analyzed'] = query['analyzed'].split(\" \")\n",
    "            query['text'] = query['text'].split(\" \")\n",
    "            query['text_unlemm'] = query['text_unlemm'].split(\" \")\n",
    "            query['text_bert_tok'] = query['text_bert_tok'].split(\" \")\n",
    "            queries[qid] = query\n",
    "    with open('queries.eval.small.Flex.json') as f:\n",
    "        for line in f:\n",
    "            query = json.loads(line)\n",
    "            qid = query.pop('id')\n",
    "            query['analyzed'] = query['analyzed'].split(\" \")\n",
    "            query['text'] = query['text'].split(\" \")\n",
    "            query['text_unlemm'] = query['text_unlemm'].split(\" \")\n",
    "            query['text_bert_tok'] = query['text_bert_tok'].split(\" \")\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "queries = query_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:37:36.359941Z",
     "start_time": "2020-12-24T05:35:18.237856Z"
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureExtractor('../indexes/msmarco-passage/lucene-index-msmarco-flex',max(multiprocessing.cpu_count()//2,1))\n",
    "fe.add(BM25(k1=0.82,b=0.68))\n",
    "fe.add(BM25(k1=0.9,b=0.4))\n",
    "fe.add(BM25(k1=1.2,b=0.75))\n",
    "fe.add(BM25(k1=2.0,b=0.75))\n",
    "\n",
    "fe.add(LMDir(mu=1000))\n",
    "fe.add(LMDir(mu=1500))\n",
    "fe.add(LMDir(mu=2500))\n",
    "\n",
    "fe.add(LMJM(0.1))\n",
    "fe.add(LMJM(0.4))\n",
    "fe.add(LMJM(0.7))\n",
    "\n",
    "fe.add(NTFIDF())\n",
    "fe.add(ProbalitySum())\n",
    "\n",
    "fe.add(DFR_GL2())\n",
    "fe.add(DFR_In_expB2())\n",
    "fe.add(DPH())\n",
    "\n",
    "fe.add(Proximity())\n",
    "fe.add(TPscore())\n",
    "fe.add(tpDist())\n",
    "\n",
    "fe.add(DocSize())\n",
    "\n",
    "fe.add(QueryLength())\n",
    "fe.add(QueryLengthNonStopWords())\n",
    "fe.add(QueryCoverageRatio())\n",
    "fe.add(UniqueTermCount())\n",
    "fe.add(MatchingTermCount())\n",
    "fe.add(SCS())\n",
    "\n",
    "fe.add(tfStat(AvgPooler()))\n",
    "fe.add(tfStat(MedianPooler()))\n",
    "fe.add(tfStat(SumPooler()))\n",
    "fe.add(tfStat(MinPooler()))\n",
    "fe.add(tfStat(MaxPooler()))\n",
    "fe.add(tfStat(VarPooler()))\n",
    "fe.add(tfStat(MaxMinRatioPooler()))\n",
    "fe.add(tfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(tfIdfStat(AvgPooler()))\n",
    "fe.add(tfIdfStat(MedianPooler()))\n",
    "fe.add(tfIdfStat(SumPooler()))\n",
    "fe.add(tfIdfStat(MinPooler()))\n",
    "fe.add(tfIdfStat(MaxPooler()))\n",
    "fe.add(tfIdfStat(VarPooler()))\n",
    "fe.add(tfIdfStat(MaxMinRatioPooler()))\n",
    "fe.add(tfIdfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(scqStat(AvgPooler()))\n",
    "fe.add(scqStat(MedianPooler()))\n",
    "fe.add(scqStat(SumPooler()))\n",
    "fe.add(scqStat(MinPooler()))\n",
    "fe.add(scqStat(MaxPooler()))\n",
    "fe.add(scqStat(VarPooler()))\n",
    "fe.add(scqStat(MaxMinRatioPooler()))\n",
    "fe.add(scqStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(normalizedTfStat(AvgPooler()))\n",
    "fe.add(normalizedTfStat(MedianPooler()))\n",
    "fe.add(normalizedTfStat(SumPooler()))\n",
    "fe.add(normalizedTfStat(MinPooler()))\n",
    "fe.add(normalizedTfStat(MaxPooler()))\n",
    "fe.add(normalizedTfStat(VarPooler()))\n",
    "fe.add(normalizedTfStat(MaxMinRatioPooler()))\n",
    "fe.add(normalizedTfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(idfStat(AvgPooler()))\n",
    "fe.add(idfStat(MedianPooler()))\n",
    "fe.add(idfStat(SumPooler()))\n",
    "fe.add(idfStat(MinPooler()))\n",
    "fe.add(idfStat(MaxPooler()))\n",
    "fe.add(idfStat(VarPooler()))\n",
    "fe.add(idfStat(MaxMinRatioPooler()))\n",
    "fe.add(idfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(ictfStat(AvgPooler()))\n",
    "fe.add(ictfStat(MedianPooler()))\n",
    "fe.add(ictfStat(SumPooler()))\n",
    "fe.add(ictfStat(MinPooler()))\n",
    "fe.add(ictfStat(MaxPooler()))\n",
    "fe.add(ictfStat(VarPooler()))\n",
    "fe.add(ictfStat(MaxMinRatioPooler()))\n",
    "fe.add(ictfStat(ConfidencePooler()))\n",
    "\n",
    "fe.add(UnorderedSequentialPairs(3))\n",
    "fe.add(UnorderedSequentialPairs(8))\n",
    "fe.add(UnorderedSequentialPairs(15))\n",
    "fe.add(OrderedSequentialPairs(3))\n",
    "fe.add(OrderedSequentialPairs(8))\n",
    "fe.add(OrderedSequentialPairs(15))\n",
    "fe.add(UnorderedQueryPairs(3))\n",
    "fe.add(UnorderedQueryPairs(8))\n",
    "fe.add(UnorderedQueryPairs(15))\n",
    "fe.add(OrderedQueryPairs(3))\n",
    "fe.add(OrderedQueryPairs(8))\n",
    "fe.add(OrderedQueryPairs(15))\n",
    "\n",
    "# fe.add(IBMModel1(\"../FlexNeuART/collections/msmarco_doc/derived_data/giza/body\",\"Unlemma\"))\n",
    "fe.add(IBMModel1(\"../FlexNeuART/collections/msmarco_doc/derived_data/giza/text_bert_tok\",\"Bert\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:37:36.373691Z",
     "start_time": "2020-12-24T05:37:36.362263Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract(df, queries, fe):\n",
    "    df_pieces = []\n",
    "    fetch_later = []\n",
    "    qidpid2rel = defaultdict(dict)\n",
    "    need_rows = 0\n",
    "    for qid,group in tqdm(df.groupby('qid')):\n",
    "        for t in group.reset_index().itertuples():\n",
    "            assert t.pid not in qidpid2rel[t.qid]\n",
    "            qidpid2rel[t.qid][t.pid] = t.rel\n",
    "            need_rows += 1\n",
    "        #test.py has bug here, it does not convert pid to str, not sure why it does not cause problem in java\n",
    "        fe.lazy_extract(str(qid),\n",
    "                        [str(pid) for pid in qidpid2rel[t.qid].keys()],\n",
    "                        queries[str(qid)])\n",
    "        fetch_later.append(str(qid))\n",
    "        if len(fetch_later) == 10000:\n",
    "            info = np.zeros(shape=(need_rows,3), dtype=np.int32)\n",
    "            feature = np.zeros(shape=(need_rows,len(fe.feature_names())), dtype=np.float32)\n",
    "            idx = 0\n",
    "            for qid in fetch_later:\n",
    "                for doc in fe.get_result(qid):\n",
    "                    info[idx,0] = int(qid)\n",
    "                    info[idx,1] = int(doc['pid'])\n",
    "                    info[idx,2] = qidpid2rel[int(qid)][int(doc['pid'])]\n",
    "                    feature[idx,:] = doc['features']\n",
    "                    idx += 1\n",
    "            info = pd.DataFrame(info, columns=['qid','pid','rel'])\n",
    "            feature = pd.DataFrame(feature, columns=fe.feature_names())\n",
    "            df_pieces.append(pd.concat([info,feature], axis=1))\n",
    "            del info, feature\n",
    "            fetch_later = []\n",
    "            need_rows = 0\n",
    "    #deal with rest\n",
    "    if len(fetch_later) > 0:\n",
    "        info = np.zeros(shape=(need_rows,3), dtype=np.int32)\n",
    "        feature = np.zeros(shape=(need_rows,len(fe.feature_names())), dtype=np.float32)\n",
    "        idx = 0\n",
    "        for qid in fetch_later:\n",
    "            for doc in fe.get_result(qid):\n",
    "                info[idx,0] = int(qid)\n",
    "                info[idx,1] = int(doc['pid'])\n",
    "                info[idx,2] = qidpid2rel[int(qid)][int(doc['pid'])]\n",
    "                feature[idx,:] = doc['features']\n",
    "                idx += 1\n",
    "        info = pd.DataFrame(info, columns=['qid','pid','rel'])\n",
    "        feature = pd.DataFrame(feature, columns=fe.feature_names())\n",
    "        df_pieces.append(pd.concat([info,feature], axis=1))\n",
    "        del info, feature\n",
    "    data = pd.concat(df_pieces, axis=0, ignore_index=True)\n",
    "    del df_pieces\n",
    "    data = data.sort_values(by='qid', kind='mergesort')\n",
    "    group = data.groupby('qid').agg(count=('pid', 'count'))['count']\n",
    "    return data,group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:37:36.410569Z",
     "start_time": "2020-12-24T05:37:36.375085Z"
    }
   },
   "outputs": [],
   "source": [
    "def hash_df(df):\n",
    "    h = pd.util.hash_pandas_object(df)\n",
    "    return hex(h.sum().astype(np.uint64))\n",
    "\n",
    "\n",
    "def hash_anserini_jar():\n",
    "    find = glob.glob(os.environ['ANSERINI_CLASSPATH'] + \"/*fatjar.jar\")\n",
    "    assert len(find) == 1\n",
    "    md5Hash = hashlib.md5(open(find[0], 'rb').read())\n",
    "    return md5Hash.hexdigest()\n",
    "\n",
    "\n",
    "def hash_fe(fe):\n",
    "    return hashlib.md5(','.join(sorted(fe.feature_names())).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def data_loader(task, df, queries, fe):\n",
    "    df_hash = hash_df(df)\n",
    "    jar_hash = hash_anserini_jar()\n",
    "    fe_hash = hash_fe(fe)\n",
    "    if os.path.exists(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle'):\n",
    "        res = pickle.load(open(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle','rb'))\n",
    "        print(res['data'].shape)\n",
    "        print(res['data'].qid.drop_duplicates().shape)\n",
    "        print(res['group'].mean())\n",
    "        print(res['data'].head(10))\n",
    "        print(res['data'].info())\n",
    "        return res\n",
    "    else:\n",
    "        if task == 'train' or task == 'dev': \n",
    "            data,group = extract(df, queries, fe)\n",
    "            obj = {'data':data,'group':group,'df_hash':df_hash,'jar_hash':jar_hash,'fe_hash':fe_hash}\n",
    "            print(data.shape)\n",
    "            print(data.qid.drop_duplicates().shape)\n",
    "            print(group.mean())\n",
    "            print(data.head(10))\n",
    "            print(data.info())\n",
    "            pickle.dump(obj,open(f'{task}_{df_hash}_{jar_hash}_{fe_hash}.pickle','wb'))\n",
    "            return obj\n",
    "        else:\n",
    "            raise Exception('unknown parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:37:36.425215Z",
     "start_time": "2020-12-24T05:37:36.414188Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def export(df, fn):\n",
    "    with open(fn,'w') as f:\n",
    "        line_num = 0\n",
    "        for qid, group in tqdm(df.groupby('qid')):\n",
    "            line = {}\n",
    "            line['qid'] = qid\n",
    "            line['docIds'] = [str(did) for did in group.reset_index().pid.drop_duplicates().tolist()]\n",
    "            assert 'qid' not in queries[str(qid)]\n",
    "            assert 'docIds' not in queries[str(qid)]\n",
    "            line.update(queries[str(qid)])\n",
    "            f.write(json.dumps(line)+'\\n')\n",
    "            line_num += 1\n",
    "            if line_num >= 1000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T05:59:32.761683Z",
     "start_time": "2020-12-24T05:37:36.429365Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400782/400782 [14:35<00:00, 457.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4416413, 89)\n",
      "(400782,)\n",
      "11.019489398226467\n",
      "   qid      pid  rel  contents_BM25_k1_0.82_b_0.68  \\\n",
      "0    3   970816    0                      13.82242   \n",
      "1    3  1142680    1                      13.82242   \n",
      "2    3  2019206    0                      13.82242   \n",
      "3    3  2605131    0                      13.82242   \n",
      "4    3  2963098    0                      13.82242   \n",
      "5    3  2971685    0                      13.82242   \n",
      "6    3  3783924    0                      13.82242   \n",
      "7    3  5067083    0                      13.82242   \n",
      "8    3  5904778    0                      13.82242   \n",
      "9    3  6176208    0                      13.82242   \n",
      "\n",
      "   contents_BM25_k1_0.90_b_0.40  contents_BM25_k1_1.20_b_0.75  \\\n",
      "0                     13.491899                     14.126501   \n",
      "1                     13.491899                     14.126501   \n",
      "2                     13.491899                     14.126501   \n",
      "3                     13.491899                     14.126501   \n",
      "4                     13.491899                     14.126501   \n",
      "5                     13.491899                     14.126501   \n",
      "6                     13.491899                     14.126501   \n",
      "7                     13.491899                     14.126501   \n",
      "8                     13.491899                     14.126501   \n",
      "9                     13.491899                     14.126501   \n",
      "\n",
      "   contents_BM25_k1_2.00_b_0.75  contents_LMD_mu_1000  contents_LMD_mu_1500  \\\n",
      "0                     14.407009            -35.664215            -36.363636   \n",
      "1                     14.407009            -35.664215            -36.363636   \n",
      "2                     14.407009            -35.664215            -36.363636   \n",
      "3                     14.407009            -35.664215            -36.363636   \n",
      "4                     14.407009            -35.664215            -36.363636   \n",
      "5                     14.407009            -35.664215            -36.363636   \n",
      "6                     14.407009            -35.664215            -36.363636   \n",
      "7                     14.407009            -35.664215            -36.363636   \n",
      "8                     14.407009            -35.664215            -36.363636   \n",
      "9                     14.407009            -35.664215            -36.363636   \n",
      "\n",
      "   contents_LMD_mu_2500  ...  contents_OrderedSequentialPairs_3  \\\n",
      "0             -37.23048  ...                                0.0   \n",
      "1             -37.23048  ...                                0.0   \n",
      "2             -37.23048  ...                                0.0   \n",
      "3             -37.23048  ...                                0.0   \n",
      "4             -37.23048  ...                                0.0   \n",
      "5             -37.23048  ...                                0.0   \n",
      "6             -37.23048  ...                                0.0   \n",
      "7             -37.23048  ...                                0.0   \n",
      "8             -37.23048  ...                                0.0   \n",
      "9             -37.23048  ...                                0.0   \n",
      "\n",
      "   contents_OrderedSequentialPairs_8  contents_OrderedSequentialPairs_15  \\\n",
      "0                                0.0                                 0.0   \n",
      "1                                0.0                                 0.0   \n",
      "2                                0.0                                 0.0   \n",
      "3                                0.0                                 0.0   \n",
      "4                                0.0                                 0.0   \n",
      "5                                0.0                                 0.0   \n",
      "6                                0.0                                 0.0   \n",
      "7                                0.0                                 0.0   \n",
      "8                                0.0                                 0.0   \n",
      "9                                0.0                                 0.0   \n",
      "\n",
      "   contents_UnorderedQueryPairs_3  contents_UnorderedQueryPairs_8  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                             0.0                             0.0   \n",
      "5                             0.0                             0.0   \n",
      "6                             0.0                             0.0   \n",
      "7                             0.0                             0.0   \n",
      "8                             0.0                             0.0   \n",
      "9                             0.0                             0.0   \n",
      "\n",
      "   contents_UnorderedQueryPairs_15  contents_OrderedQueryPairs_3  \\\n",
      "0                              0.0                           0.0   \n",
      "1                              0.0                           0.0   \n",
      "2                              0.0                           0.0   \n",
      "3                              0.0                           0.0   \n",
      "4                              0.0                           0.0   \n",
      "5                              0.0                           0.0   \n",
      "6                              0.0                           0.0   \n",
      "7                              0.0                           0.0   \n",
      "8                              0.0                           0.0   \n",
      "9                              0.0                           0.0   \n",
      "\n",
      "   contents_OrderedQueryPairs_8  contents_OrderedQueryPairs_15  Bert_IBMModel1  \n",
      "0                           0.0                            0.0             0.0  \n",
      "1                           0.0                            0.0             0.0  \n",
      "2                           0.0                            0.0             0.0  \n",
      "3                           0.0                            0.0             0.0  \n",
      "4                           0.0                            0.0             0.0  \n",
      "5                           0.0                            0.0             0.0  \n",
      "6                           0.0                            0.0             0.0  \n",
      "7                           0.0                            0.0             0.0  \n",
      "8                           0.0                            0.0             0.0  \n",
      "9                           0.0                            0.0             0.0  \n",
      "\n",
      "[10 rows x 89 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4416413 entries, 0 to 4416412\n",
      "Data columns (total 89 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   qid                                   int32  \n",
      " 1   pid                                   int32  \n",
      " 2   rel                                   int32  \n",
      " 3   contents_BM25_k1_0.82_b_0.68          float32\n",
      " 4   contents_BM25_k1_0.90_b_0.40          float32\n",
      " 5   contents_BM25_k1_1.20_b_0.75          float32\n",
      " 6   contents_BM25_k1_2.00_b_0.75          float32\n",
      " 7   contents_LMD_mu_1000                  float32\n",
      " 8   contents_LMD_mu_1500                  float32\n",
      " 9   contents_LMD_mu_2500                  float32\n",
      " 10  contents_LMJM_lambda_0.10             float32\n",
      " 11  contents_LMJM_lambda_0.40             float32\n",
      " 12  contents_LMJM_lambda_0.70             float32\n",
      " 13  contents_NTFIDF                       float32\n",
      " 14  contents_Prob                         float32\n",
      " 15  contents_DFR_GL2                      float32\n",
      " 16  contents_DFR_In_expB2                 float32\n",
      " 17  contents_DPH                          float32\n",
      " 18  contents_Proximity                    float32\n",
      " 19  contents_TPscore                      float32\n",
      " 20  contents_tpDistWindow100              float32\n",
      " 21  contents_DocSize                      float32\n",
      " 22  QueryLength                           float32\n",
      " 23  QueryLengthNonStopWords               float32\n",
      " 24  contents_QueryCoverageRatio           float32\n",
      " 25  UniqueQueryTerms                      float32\n",
      " 26  contents_MatchingTermCount            float32\n",
      " 27  contents_SCS                          float32\n",
      " 28  contents_TF_avg                       float32\n",
      " 29  contents_TF_median                    float32\n",
      " 30  contents_TF_sum                       float32\n",
      " 31  contents_TF_min                       float32\n",
      " 32  contents_TF_max                       float32\n",
      " 33  contents_TF_var                       float32\n",
      " 34  contents_TF_maxminratio               float32\n",
      " 35  contents_TF_confidence                float32\n",
      " 36  contents_TFIDF_avg                    float32\n",
      " 37  contents_TFIDF_median                 float32\n",
      " 38  contents_TFIDF_sum                    float32\n",
      " 39  contents_TFIDF_min                    float32\n",
      " 40  contents_TFIDF_max                    float32\n",
      " 41  contents_TFIDF_var                    float32\n",
      " 42  contents_TFIDF_maxminratio            float32\n",
      " 43  contents_TFIDF_confidence             float32\n",
      " 44  contents_SCQ_avg                      float32\n",
      " 45  contents_SCQ_median                   float32\n",
      " 46  contents_SCQ_sum                      float32\n",
      " 47  contents_SCQ_min                      float32\n",
      " 48  contents_SCQ_max                      float32\n",
      " 49  contents_SCQ_var                      float32\n",
      " 50  contents_SCQ_maxminratio              float32\n",
      " 51  contents_SCQ_confidence               float32\n",
      " 52  contents_NormalizedTF_avg             float32\n",
      " 53  contents_NormalizedTF_median          float32\n",
      " 54  contents_NormalizedTF_sum             float32\n",
      " 55  contents_NormalizedTF_min             float32\n",
      " 56  contents_NormalizedTF_max             float32\n",
      " 57  contents_NormalizedTF_var             float32\n",
      " 58  contents_NormalizedTF_maxminratio     float32\n",
      " 59  contents_NormalizedTF_confidence      float32\n",
      " 60  contents_IDF_avg                      float32\n",
      " 61  contents_IDF_median                   float32\n",
      " 62  contents_IDF_sum                      float32\n",
      " 63  contents_IDF_min                      float32\n",
      " 64  contents_IDF_max                      float32\n",
      " 65  contents_IDF_var                      float32\n",
      " 66  contents_IDF_maxminratio              float32\n",
      " 67  contents_IDF_confidence               float32\n",
      " 68  contents_ICTF_avg                     float32\n",
      " 69  contents_ICTF_median                  float32\n",
      " 70  contents_ICTF_sum                     float32\n",
      " 71  contents_ICTF_min                     float32\n",
      " 72  contents_ICTF_max                     float32\n",
      " 73  contents_ICTF_var                     float32\n",
      " 74  contents_ICTF_maxminratio             float32\n",
      " 75  contents_ICTF_confidence              float32\n",
      " 76  contents_UnorderedSequentialPairs_3   float32\n",
      " 77  contents_UnorderedSequentialPairs_8   float32\n",
      " 78  contents_UnorderedSequentialPairs_15  float32\n",
      " 79  contents_OrderedSequentialPairs_3     float32\n",
      " 80  contents_OrderedSequentialPairs_8     float32\n",
      " 81  contents_OrderedSequentialPairs_15    float32\n",
      " 82  contents_UnorderedQueryPairs_3        float32\n",
      " 83  contents_UnorderedQueryPairs_8        float32\n",
      " 84  contents_UnorderedQueryPairs_15       float32\n",
      " 85  contents_OrderedQueryPairs_3          float32\n",
      " 86  contents_OrderedQueryPairs_8          float32\n",
      " 87  contents_OrderedQueryPairs_15         float32\n",
      " 88  Bert_IBMModel1                        float32\n",
      "dtypes: float32(86), int32(3)\n",
      "memory usage: 1.7 GB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:40<00:00, 170.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6974598, 89)\n",
      "(6980,)\n",
      "999.2260744985673\n",
      "   qid     pid  rel  contents_BM25_k1_0.82_b_0.68  \\\n",
      "0    2   55860    0                     10.291057   \n",
      "1    2   72202    0                     10.291057   \n",
      "2    2   72210    0                     10.291057   \n",
      "3    2   98589    0                     10.291057   \n",
      "4    2   98590    0                     10.291057   \n",
      "5    2   98593    0                     10.291057   \n",
      "6    2   98595    0                     10.291057   \n",
      "7    2  112123    0                     10.291057   \n",
      "8    2  112126    0                     10.291057   \n",
      "9    2  112127    0                     10.291057   \n",
      "\n",
      "   contents_BM25_k1_0.90_b_0.40  contents_BM25_k1_1.20_b_0.75  \\\n",
      "0                     10.486095                     11.856373   \n",
      "1                     10.486095                     11.856373   \n",
      "2                     10.486095                     11.856373   \n",
      "3                     10.486095                     11.856373   \n",
      "4                     10.486095                     11.856373   \n",
      "5                     10.486095                     11.856373   \n",
      "6                     10.486095                     11.856373   \n",
      "7                     10.486095                     11.856373   \n",
      "8                     10.486095                     11.856373   \n",
      "9                     10.486095                     11.856373   \n",
      "\n",
      "   contents_BM25_k1_2.00_b_0.75  contents_LMD_mu_1000  contents_LMD_mu_1500  \\\n",
      "0                      14.63435            -25.113516            -25.484682   \n",
      "1                      14.63435            -25.113516            -25.484682   \n",
      "2                      14.63435            -25.113516            -25.484682   \n",
      "3                      14.63435            -25.113516            -25.484682   \n",
      "4                      14.63435            -25.113516            -25.484682   \n",
      "5                      14.63435            -25.113516            -25.484682   \n",
      "6                      14.63435            -25.113516            -25.484682   \n",
      "7                      14.63435            -25.113516            -25.484682   \n",
      "8                      14.63435            -25.113516            -25.484682   \n",
      "9                      14.63435            -25.113516            -25.484682   \n",
      "\n",
      "   contents_LMD_mu_2500  ...  contents_OrderedSequentialPairs_3  \\\n",
      "0            -25.959703  ...                                0.0   \n",
      "1            -25.959703  ...                                0.0   \n",
      "2            -25.959703  ...                                0.0   \n",
      "3            -25.959703  ...                                0.0   \n",
      "4            -25.959703  ...                                0.0   \n",
      "5            -25.959703  ...                                0.0   \n",
      "6            -25.959703  ...                                0.0   \n",
      "7            -25.959703  ...                                0.0   \n",
      "8            -25.959703  ...                                0.0   \n",
      "9            -25.959703  ...                                0.0   \n",
      "\n",
      "   contents_OrderedSequentialPairs_8  contents_OrderedSequentialPairs_15  \\\n",
      "0                                0.0                                 0.0   \n",
      "1                                0.0                                 0.0   \n",
      "2                                0.0                                 0.0   \n",
      "3                                0.0                                 0.0   \n",
      "4                                0.0                                 0.0   \n",
      "5                                0.0                                 0.0   \n",
      "6                                0.0                                 0.0   \n",
      "7                                0.0                                 0.0   \n",
      "8                                0.0                                 0.0   \n",
      "9                                0.0                                 0.0   \n",
      "\n",
      "   contents_UnorderedQueryPairs_3  contents_UnorderedQueryPairs_8  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                             0.0                             0.0   \n",
      "5                             0.0                             0.0   \n",
      "6                             0.0                             0.0   \n",
      "7                             0.0                             0.0   \n",
      "8                             0.0                             0.0   \n",
      "9                             0.0                             0.0   \n",
      "\n",
      "   contents_UnorderedQueryPairs_15  contents_OrderedQueryPairs_3  \\\n",
      "0                              0.0                           0.0   \n",
      "1                              0.0                           0.0   \n",
      "2                              0.0                           0.0   \n",
      "3                              0.0                           0.0   \n",
      "4                              0.0                           0.0   \n",
      "5                              0.0                           0.0   \n",
      "6                              0.0                           0.0   \n",
      "7                              0.0                           0.0   \n",
      "8                              0.0                           0.0   \n",
      "9                              0.0                           0.0   \n",
      "\n",
      "   contents_OrderedQueryPairs_8  contents_OrderedQueryPairs_15  Bert_IBMModel1  \n",
      "0                           0.0                            0.0             0.0  \n",
      "1                           0.0                            0.0             0.0  \n",
      "2                           0.0                            0.0             0.0  \n",
      "3                           0.0                            0.0             0.0  \n",
      "4                           0.0                            0.0             0.0  \n",
      "5                           0.0                            0.0             0.0  \n",
      "6                           0.0                            0.0             0.0  \n",
      "7                           0.0                            0.0             0.0  \n",
      "8                           0.0                            0.0             0.0  \n",
      "9                           0.0                            0.0             0.0  \n",
      "\n",
      "[10 rows x 89 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6974598 entries, 0 to 6974597\n",
      "Data columns (total 89 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   qid                                   int32  \n",
      " 1   pid                                   int32  \n",
      " 2   rel                                   int32  \n",
      " 3   contents_BM25_k1_0.82_b_0.68          float32\n",
      " 4   contents_BM25_k1_0.90_b_0.40          float32\n",
      " 5   contents_BM25_k1_1.20_b_0.75          float32\n",
      " 6   contents_BM25_k1_2.00_b_0.75          float32\n",
      " 7   contents_LMD_mu_1000                  float32\n",
      " 8   contents_LMD_mu_1500                  float32\n",
      " 9   contents_LMD_mu_2500                  float32\n",
      " 10  contents_LMJM_lambda_0.10             float32\n",
      " 11  contents_LMJM_lambda_0.40             float32\n",
      " 12  contents_LMJM_lambda_0.70             float32\n",
      " 13  contents_NTFIDF                       float32\n",
      " 14  contents_Prob                         float32\n",
      " 15  contents_DFR_GL2                      float32\n",
      " 16  contents_DFR_In_expB2                 float32\n",
      " 17  contents_DPH                          float32\n",
      " 18  contents_Proximity                    float32\n",
      " 19  contents_TPscore                      float32\n",
      " 20  contents_tpDistWindow100              float32\n",
      " 21  contents_DocSize                      float32\n",
      " 22  QueryLength                           float32\n",
      " 23  QueryLengthNonStopWords               float32\n",
      " 24  contents_QueryCoverageRatio           float32\n",
      " 25  UniqueQueryTerms                      float32\n",
      " 26  contents_MatchingTermCount            float32\n",
      " 27  contents_SCS                          float32\n",
      " 28  contents_TF_avg                       float32\n",
      " 29  contents_TF_median                    float32\n",
      " 30  contents_TF_sum                       float32\n",
      " 31  contents_TF_min                       float32\n",
      " 32  contents_TF_max                       float32\n",
      " 33  contents_TF_var                       float32\n",
      " 34  contents_TF_maxminratio               float32\n",
      " 35  contents_TF_confidence                float32\n",
      " 36  contents_TFIDF_avg                    float32\n",
      " 37  contents_TFIDF_median                 float32\n",
      " 38  contents_TFIDF_sum                    float32\n",
      " 39  contents_TFIDF_min                    float32\n",
      " 40  contents_TFIDF_max                    float32\n",
      " 41  contents_TFIDF_var                    float32\n",
      " 42  contents_TFIDF_maxminratio            float32\n",
      " 43  contents_TFIDF_confidence             float32\n",
      " 44  contents_SCQ_avg                      float32\n",
      " 45  contents_SCQ_median                   float32\n",
      " 46  contents_SCQ_sum                      float32\n",
      " 47  contents_SCQ_min                      float32\n",
      " 48  contents_SCQ_max                      float32\n",
      " 49  contents_SCQ_var                      float32\n",
      " 50  contents_SCQ_maxminratio              float32\n",
      " 51  contents_SCQ_confidence               float32\n",
      " 52  contents_NormalizedTF_avg             float32\n",
      " 53  contents_NormalizedTF_median          float32\n",
      " 54  contents_NormalizedTF_sum             float32\n",
      " 55  contents_NormalizedTF_min             float32\n",
      " 56  contents_NormalizedTF_max             float32\n",
      " 57  contents_NormalizedTF_var             float32\n",
      " 58  contents_NormalizedTF_maxminratio     float32\n",
      " 59  contents_NormalizedTF_confidence      float32\n",
      " 60  contents_IDF_avg                      float32\n",
      " 61  contents_IDF_median                   float32\n",
      " 62  contents_IDF_sum                      float32\n",
      " 63  contents_IDF_min                      float32\n",
      " 64  contents_IDF_max                      float32\n",
      " 65  contents_IDF_var                      float32\n",
      " 66  contents_IDF_maxminratio              float32\n",
      " 67  contents_IDF_confidence               float32\n",
      " 68  contents_ICTF_avg                     float32\n",
      " 69  contents_ICTF_median                  float32\n",
      " 70  contents_ICTF_sum                     float32\n",
      " 71  contents_ICTF_min                     float32\n",
      " 72  contents_ICTF_max                     float32\n",
      " 73  contents_ICTF_var                     float32\n",
      " 74  contents_ICTF_maxminratio             float32\n",
      " 75  contents_ICTF_confidence              float32\n",
      " 76  contents_UnorderedSequentialPairs_3   float32\n",
      " 77  contents_UnorderedSequentialPairs_8   float32\n",
      " 78  contents_UnorderedSequentialPairs_15  float32\n",
      " 79  contents_OrderedSequentialPairs_3     float32\n",
      " 80  contents_OrderedSequentialPairs_8     float32\n",
      " 81  contents_OrderedSequentialPairs_15    float32\n",
      " 82  contents_UnorderedQueryPairs_3        float32\n",
      " 83  contents_UnorderedQueryPairs_8        float32\n",
      " 84  contents_UnorderedQueryPairs_15       float32\n",
      " 85  contents_OrderedQueryPairs_3          float32\n",
      " 86  contents_OrderedQueryPairs_8          float32\n",
      " 87  contents_OrderedQueryPairs_15         float32\n",
      " 88  Bert_IBMModel1                        float32\n",
      "dtypes: float32(86), int32(3)\n",
      "memory usage: 2.7 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_extracted = data_loader('train', sampled_train, queries, fe)\n",
    "# export(sampled_train, 'sampled_train_export.json')\n",
    "dev_extracted = data_loader('dev', dev, queries, fe)\n",
    "# export(dev, 'sampled_dev_export.json')\n",
    "del sampled_train, dev\n",
    "feature_name = fe.feature_names()\n",
    "del queries, fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:06:56.005832Z",
     "start_time": "2020-12-24T06:06:55.989321Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_mrr(dev_data):\n",
    "    score_tie_counter = 0\n",
    "    score_tie_query = set()\n",
    "\n",
    "    MRR = []\n",
    "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
    "        group = group.reset_index()\n",
    "        rank = 0\n",
    "        prev_score = None\n",
    "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
    "        # stable sort is also used in LightGBM\n",
    "\n",
    "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
    "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
    "                score_tie_counter += 1\n",
    "                score_tie_query.add(qid)\n",
    "            prev_score = t.score\n",
    "            prev_pid = t.pid\n",
    "            rank += 1\n",
    "            if t.rel>0:\n",
    "                MRR.append(1.0/rank)\n",
    "                break\n",
    "            elif rank == 10 or rank == len(group):\n",
    "                MRR.append(0.)\n",
    "                break\n",
    "\n",
    "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
    "    print(score_tie,np.mean(MRR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:06:56.344013Z",
     "start_time": "2020-12-24T06:06:56.321800Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_recall(dev_qrel, dev_data):\n",
    "    dev_rel_num = dev_qrel[dev_qrel['rel']>0].groupby('qid').count()['rel']\n",
    "\n",
    "    score_tie_counter = 0\n",
    "    score_tie_query = set()\n",
    "    \n",
    "    recall_point = [10,20,50,100,200,500,1000]\n",
    "    recall_curve = {k:[] for k in recall_point}\n",
    "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
    "        group = group.reset_index()\n",
    "        rank = 0\n",
    "        prev_score = None\n",
    "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
    "        # stable sort is also used in LightGBM\n",
    "        total_rel = dev_rel_num.loc[qid]\n",
    "        query_recall = [0 for k in recall_point]\n",
    "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
    "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
    "                score_tie_counter += 1\n",
    "                score_tie_query.add(qid)\n",
    "            prev_score = t.score\n",
    "            rank += 1\n",
    "            if t.rel>0:\n",
    "                for i,p in enumerate(recall_point):\n",
    "                    if rank <= p:\n",
    "                        query_recall[i] += 1\n",
    "        for i,p in enumerate(recall_point):\n",
    "            if total_rel>0:\n",
    "                recall_curve[p].append(query_recall[i]/total_rel)\n",
    "            else:\n",
    "                recall_curve[p].append(0.)\n",
    "\n",
    "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
    "    print(score_tie)\n",
    "    \n",
    "    for k,v in recall_curve.items():\n",
    "        avg = np.mean(v)\n",
    "        print(f'recall@{k}:{avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:07:52.125880Z",
     "start_time": "2020-12-24T06:06:57.077069Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_X = train_extracted['data'].loc[:, feature_name]\n",
    "train_Y = train_extracted['data']['rel']\n",
    "dev_X = dev_extracted['data'].loc[:, feature_name]\n",
    "dev_Y = dev_extracted['data']['rel']\n",
    "lgb_train = lgb.Dataset(train_X,label=train_Y,group=train_extracted['group'])\n",
    "lgb_valid = lgb.Dataset(dev_X,label=dev_Y,group=dev_extracted['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:07:52.402359Z",
     "start_time": "2020-12-24T06:07:52.141635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents_TFIDF_confidence\n",
      "contents_SCQ_confidence\n",
      "contents_NormalizedTF_confidence\n",
      "contents_IDF_confidence\n",
      "contents_ICTF_confidence\n"
     ]
    }
   ],
   "source": [
    "for i,n in enumerate(feature_name):\n",
    "    if np.isnan(train_X.iloc[:,i]).any():\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:08:13.572910Z",
     "start_time": "2020-12-24T06:07:52.405030Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_rel_num = dev_qrel[dev_qrel['rel']>0].groupby('qid').count()['rel']\n",
    "gid = 0\n",
    "sample_id = 0\n",
    "dev_group_rel_num = []\n",
    "for qid,group in dev_extracted['data'].groupby('qid'):\n",
    "    group = group.sort_values(['pid'])\n",
    "    assert len(group) == dev_extracted['group'].iloc[gid]\n",
    "    assert np.isclose(group.iloc[0,:].loc[feature_name],\n",
    "                      dev_X.iloc[sample_id,:], equal_nan=True).all()\n",
    "    dev_group_rel_num.append(dev_rel_num.loc[qid])\n",
    "    gid += 1\n",
    "    sample_id += len(group)\n",
    "dev_group = dev_extracted['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:08:13.580008Z",
     "start_time": "2020-12-24T06:08:13.575211Z"
    }
   },
   "outputs": [],
   "source": [
    "def recall_at_200(preds, dataset):\n",
    "    global dev_group_rel_num\n",
    "    global dev_group\n",
    "    labels = dataset.get_label()\n",
    "    groups = dataset.get_group()\n",
    "    assert np.equal(groups, dev_group).all()\n",
    "    idx = 0\n",
    "    recall = 0\n",
    "    for g,gnum in zip(groups, dev_group_rel_num):\n",
    "        top_preds = labels[idx:idx+g][np.argsort(preds[idx:idx+g])]\n",
    "        recall += np.sum(top_preds[-200:])/gnum\n",
    "        idx += g\n",
    "    assert idx == len(preds)\n",
    "    return 'recall@200', recall/len(groups), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T06:12:46.484607Z",
     "start_time": "2020-12-24T06:08:13.581992Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.326444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 15589\n",
      "[LightGBM] [Info] Number of data points in the train set: 4416413, number of used features: 79\n",
      "[1]\tvalid_0's recall@200: 0.0316858\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\tvalid_0's recall@200: 0.0316858\n",
      "[3]\tvalid_0's recall@200: 0.0316858\n",
      "[4]\tvalid_0's recall@200: 0.0316858\n",
      "[5]\tvalid_0's recall@200: 0.0316858\n",
      "[6]\tvalid_0's recall@200: 0.0316858\n",
      "[7]\tvalid_0's recall@200: 0.0316858\n",
      "[8]\tvalid_0's recall@200: 0.0316858\n",
      "[9]\tvalid_0's recall@200: 0.0316858\n",
      "[10]\tvalid_0's recall@200: 0.0316858\n",
      "[11]\tvalid_0's recall@200: 0.0316858\n",
      "[12]\tvalid_0's recall@200: 0.0316858\n",
      "[13]\tvalid_0's recall@200: 0.0316858\n",
      "[14]\tvalid_0's recall@200: 0.0316858\n",
      "[15]\tvalid_0's recall@200: 0.0316858\n",
      "[16]\tvalid_0's recall@200: 0.0316858\n",
      "[17]\tvalid_0's recall@200: 0.0316858\n",
      "[18]\tvalid_0's recall@200: 0.0316858\n",
      "[19]\tvalid_0's recall@200: 0.0316858\n",
      "[20]\tvalid_0's recall@200: 0.0316858\n",
      "[21]\tvalid_0's recall@200: 0.0316858\n",
      "[22]\tvalid_0's recall@200: 0.0316858\n",
      "[23]\tvalid_0's recall@200: 0.0316858\n",
      "[24]\tvalid_0's recall@200: 0.0316858\n",
      "[25]\tvalid_0's recall@200: 0.0316858\n",
      "[26]\tvalid_0's recall@200: 0.0316858\n",
      "[27]\tvalid_0's recall@200: 0.0316858\n",
      "[28]\tvalid_0's recall@200: 0.0316858\n",
      "[29]\tvalid_0's recall@200: 0.0316858\n",
      "[30]\tvalid_0's recall@200: 0.0316858\n",
      "[31]\tvalid_0's recall@200: 0.0316858\n",
      "[32]\tvalid_0's recall@200: 0.0316858\n",
      "[33]\tvalid_0's recall@200: 0.0316858\n",
      "[34]\tvalid_0's recall@200: 0.0316858\n",
      "[35]\tvalid_0's recall@200: 0.0316858\n",
      "[36]\tvalid_0's recall@200: 0.0316858\n",
      "[37]\tvalid_0's recall@200: 0.0316858\n",
      "[38]\tvalid_0's recall@200: 0.0316858\n",
      "[39]\tvalid_0's recall@200: 0.0316858\n",
      "[40]\tvalid_0's recall@200: 0.0316858\n",
      "[41]\tvalid_0's recall@200: 0.0316858\n",
      "[42]\tvalid_0's recall@200: 0.0316858\n",
      "[43]\tvalid_0's recall@200: 0.0316858\n",
      "[44]\tvalid_0's recall@200: 0.0316858\n",
      "[45]\tvalid_0's recall@200: 0.0316858\n",
      "[46]\tvalid_0's recall@200: 0.0316858\n",
      "[47]\tvalid_0's recall@200: 0.0316858\n",
      "[48]\tvalid_0's recall@200: 0.0316858\n",
      "[49]\tvalid_0's recall@200: 0.0316858\n",
      "[50]\tvalid_0's recall@200: 0.0316858\n",
      "[51]\tvalid_0's recall@200: 0.0316858\n",
      "[52]\tvalid_0's recall@200: 0.0316858\n",
      "[53]\tvalid_0's recall@200: 0.0316858\n",
      "[54]\tvalid_0's recall@200: 0.0316858\n",
      "[55]\tvalid_0's recall@200: 0.0316858\n",
      "[56]\tvalid_0's recall@200: 0.0316858\n",
      "[57]\tvalid_0's recall@200: 0.0316858\n",
      "[58]\tvalid_0's recall@200: 0.0316858\n",
      "[59]\tvalid_0's recall@200: 0.0316858\n",
      "[60]\tvalid_0's recall@200: 0.0316858\n",
      "[61]\tvalid_0's recall@200: 0.0316858\n",
      "[62]\tvalid_0's recall@200: 0.0316858\n",
      "[63]\tvalid_0's recall@200: 0.0316858\n",
      "[64]\tvalid_0's recall@200: 0.0316858\n",
      "[65]\tvalid_0's recall@200: 0.0316858\n",
      "[66]\tvalid_0's recall@200: 0.0316858\n",
      "[67]\tvalid_0's recall@200: 0.0316858\n",
      "[68]\tvalid_0's recall@200: 0.0316858\n",
      "[69]\tvalid_0's recall@200: 0.0316858\n",
      "[70]\tvalid_0's recall@200: 0.0316858\n",
      "[71]\tvalid_0's recall@200: 0.0316858\n",
      "[72]\tvalid_0's recall@200: 0.0316858\n",
      "[73]\tvalid_0's recall@200: 0.0316858\n",
      "[74]\tvalid_0's recall@200: 0.0316858\n",
      "[75]\tvalid_0's recall@200: 0.0316858\n",
      "[76]\tvalid_0's recall@200: 0.0316858\n",
      "[77]\tvalid_0's recall@200: 0.0316858\n",
      "[78]\tvalid_0's recall@200: 0.0316858\n",
      "[79]\tvalid_0's recall@200: 0.0316858\n",
      "[80]\tvalid_0's recall@200: 0.0316858\n",
      "[81]\tvalid_0's recall@200: 0.0316858\n",
      "[82]\tvalid_0's recall@200: 0.0316858\n",
      "[83]\tvalid_0's recall@200: 0.0316858\n",
      "[84]\tvalid_0's recall@200: 0.0316858\n",
      "[85]\tvalid_0's recall@200: 0.0316858\n",
      "[86]\tvalid_0's recall@200: 0.0316858\n",
      "[87]\tvalid_0's recall@200: 0.0316858\n",
      "[88]\tvalid_0's recall@200: 0.0316858\n",
      "[89]\tvalid_0's recall@200: 0.0316858\n",
      "[90]\tvalid_0's recall@200: 0.0316858\n",
      "[91]\tvalid_0's recall@200: 0.0316858\n",
      "[92]\tvalid_0's recall@200: 0.0316858\n",
      "[93]\tvalid_0's recall@200: 0.0316858\n",
      "[94]\tvalid_0's recall@200: 0.0316858\n",
      "[95]\tvalid_0's recall@200: 0.0316858\n",
      "[96]\tvalid_0's recall@200: 0.0316858\n",
      "[97]\tvalid_0's recall@200: 0.0316858\n",
      "[98]\tvalid_0's recall@200: 0.0316858\n",
      "[99]\tvalid_0's recall@200: 0.0316858\n",
      "[100]\tvalid_0's recall@200: 0.0316858\n",
      "[101]\tvalid_0's recall@200: 0.0316858\n",
      "[102]\tvalid_0's recall@200: 0.0316858\n",
      "[103]\tvalid_0's recall@200: 0.0316858\n",
      "[104]\tvalid_0's recall@200: 0.0316858\n",
      "[105]\tvalid_0's recall@200: 0.0316858\n",
      "[106]\tvalid_0's recall@200: 0.0316858\n",
      "[107]\tvalid_0's recall@200: 0.0316858\n",
      "[108]\tvalid_0's recall@200: 0.0316858\n",
      "[109]\tvalid_0's recall@200: 0.0316858\n",
      "[110]\tvalid_0's recall@200: 0.0316858\n",
      "[111]\tvalid_0's recall@200: 0.0316858\n",
      "[112]\tvalid_0's recall@200: 0.0316858\n",
      "[113]\tvalid_0's recall@200: 0.0316858\n",
      "[114]\tvalid_0's recall@200: 0.0316858\n",
      "[115]\tvalid_0's recall@200: 0.0316858\n",
      "[116]\tvalid_0's recall@200: 0.0316858\n",
      "[117]\tvalid_0's recall@200: 0.0316858\n",
      "[118]\tvalid_0's recall@200: 0.0316858\n",
      "[119]\tvalid_0's recall@200: 0.0316858\n",
      "[120]\tvalid_0's recall@200: 0.0316858\n",
      "[121]\tvalid_0's recall@200: 0.0316858\n",
      "[122]\tvalid_0's recall@200: 0.0316858\n",
      "[123]\tvalid_0's recall@200: 0.0316858\n",
      "[124]\tvalid_0's recall@200: 0.0316858\n",
      "[125]\tvalid_0's recall@200: 0.0316858\n",
      "[126]\tvalid_0's recall@200: 0.0316858\n",
      "[127]\tvalid_0's recall@200: 0.0316858\n",
      "[128]\tvalid_0's recall@200: 0.0316858\n",
      "[129]\tvalid_0's recall@200: 0.0316858\n",
      "[130]\tvalid_0's recall@200: 0.0316858\n",
      "[131]\tvalid_0's recall@200: 0.0316858\n",
      "[132]\tvalid_0's recall@200: 0.0316858\n",
      "[133]\tvalid_0's recall@200: 0.0316858\n",
      "[134]\tvalid_0's recall@200: 0.0316858\n",
      "[135]\tvalid_0's recall@200: 0.0316858\n",
      "[136]\tvalid_0's recall@200: 0.0316858\n",
      "[137]\tvalid_0's recall@200: 0.0316858\n",
      "[138]\tvalid_0's recall@200: 0.0316858\n",
      "[139]\tvalid_0's recall@200: 0.0316858\n",
      "[140]\tvalid_0's recall@200: 0.0316858\n",
      "[141]\tvalid_0's recall@200: 0.0316858\n",
      "[142]\tvalid_0's recall@200: 0.0316858\n",
      "[143]\tvalid_0's recall@200: 0.0316858\n",
      "[144]\tvalid_0's recall@200: 0.0316858\n",
      "[145]\tvalid_0's recall@200: 0.0316858\n",
      "[146]\tvalid_0's recall@200: 0.0316858\n",
      "[147]\tvalid_0's recall@200: 0.0316858\n",
      "[148]\tvalid_0's recall@200: 0.0316858\n",
      "[149]\tvalid_0's recall@200: 0.0316858\n",
      "[150]\tvalid_0's recall@200: 0.0316858\n",
      "[151]\tvalid_0's recall@200: 0.0316858\n",
      "[152]\tvalid_0's recall@200: 0.0316858\n",
      "[153]\tvalid_0's recall@200: 0.0316858\n",
      "[154]\tvalid_0's recall@200: 0.0316858\n",
      "[155]\tvalid_0's recall@200: 0.0316858\n",
      "[156]\tvalid_0's recall@200: 0.0316858\n",
      "[157]\tvalid_0's recall@200: 0.0316858\n",
      "[158]\tvalid_0's recall@200: 0.0316858\n",
      "[159]\tvalid_0's recall@200: 0.0316858\n",
      "[160]\tvalid_0's recall@200: 0.0316858\n",
      "[161]\tvalid_0's recall@200: 0.0316858\n",
      "[162]\tvalid_0's recall@200: 0.0316858\n",
      "[163]\tvalid_0's recall@200: 0.0316858\n",
      "[164]\tvalid_0's recall@200: 0.0316858\n",
      "[165]\tvalid_0's recall@200: 0.0316858\n",
      "[166]\tvalid_0's recall@200: 0.0316858\n",
      "[167]\tvalid_0's recall@200: 0.0316858\n",
      "[168]\tvalid_0's recall@200: 0.0316858\n",
      "[169]\tvalid_0's recall@200: 0.0316858\n",
      "[170]\tvalid_0's recall@200: 0.0316858\n",
      "[171]\tvalid_0's recall@200: 0.0316858\n",
      "[172]\tvalid_0's recall@200: 0.0316858\n",
      "[173]\tvalid_0's recall@200: 0.0316858\n",
      "[174]\tvalid_0's recall@200: 0.0316858\n",
      "[175]\tvalid_0's recall@200: 0.0316858\n",
      "[176]\tvalid_0's recall@200: 0.0316858\n",
      "[177]\tvalid_0's recall@200: 0.0316858\n",
      "[178]\tvalid_0's recall@200: 0.0316858\n",
      "[179]\tvalid_0's recall@200: 0.0316858\n",
      "[180]\tvalid_0's recall@200: 0.0316858\n",
      "[181]\tvalid_0's recall@200: 0.0316858\n",
      "[182]\tvalid_0's recall@200: 0.0316858\n",
      "[183]\tvalid_0's recall@200: 0.0316858\n",
      "[184]\tvalid_0's recall@200: 0.0316858\n",
      "[185]\tvalid_0's recall@200: 0.0316858\n",
      "[186]\tvalid_0's recall@200: 0.0316858\n",
      "[187]\tvalid_0's recall@200: 0.0316858\n",
      "[188]\tvalid_0's recall@200: 0.0316858\n",
      "[189]\tvalid_0's recall@200: 0.0316858\n",
      "[190]\tvalid_0's recall@200: 0.0316858\n",
      "[191]\tvalid_0's recall@200: 0.0316858\n",
      "[192]\tvalid_0's recall@200: 0.0316858\n",
      "[193]\tvalid_0's recall@200: 0.0316858\n",
      "[194]\tvalid_0's recall@200: 0.0316858\n",
      "[195]\tvalid_0's recall@200: 0.0316858\n",
      "[196]\tvalid_0's recall@200: 0.0316858\n",
      "[197]\tvalid_0's recall@200: 0.0316858\n",
      "[198]\tvalid_0's recall@200: 0.0316858\n",
      "[199]\tvalid_0's recall@200: 0.0316858\n",
      "[200]\tvalid_0's recall@200: 0.0316858\n",
      "[201]\tvalid_0's recall@200: 0.0316858\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's recall@200: 0.0316858\n",
      "0.03168576886341929\n",
      "1\n",
      "[('contents_IDF_max', 8), ('contents_IDF_sum', 6), ('contents_SCQ_sum', 5), ('contents_BM25_k1_0.82_b_0.68', 2), ('contents_DFR_GL2', 2), ('contents_DPH', 2), ('contents_TPscore', 2), ('contents_SCQ_var', 2), ('contents_SCQ_maxminratio', 2), ('contents_NormalizedTF_confidence', 2), ('contents_IDF_median', 2), ('contents_ICTF_avg', 2), ('contents_ICTF_maxminratio', 2), ('contents_BM25_k1_1.20_b_0.75', 1), ('contents_BM25_k1_2.00_b_0.75', 1), ('contents_LMJM_lambda_0.10', 1), ('contents_LMJM_lambda_0.40', 1), ('contents_DFR_In_expB2', 1), ('contents_DocSize', 1), ('QueryLengthNonStopWords', 1), ('contents_SCS', 1), ('contents_TF_confidence', 1), ('contents_TFIDF_avg', 1), ('contents_TFIDF_median', 1), ('contents_TFIDF_sum', 1), ('contents_TFIDF_max', 1), ('contents_SCQ_avg', 1), ('contents_SCQ_median', 1), ('contents_SCQ_confidence', 1), ('contents_NormalizedTF_var', 1), ('contents_NormalizedTF_maxminratio', 1), ('contents_IDF_var', 1), ('contents_IDF_confidence', 1), ('contents_ICTF_median', 1), ('contents_ICTF_var', 1), ('contents_OrderedQueryPairs_3', 1), ('contents_BM25_k1_0.90_b_0.40', 0), ('contents_LMD_mu_1000', 0), ('contents_LMD_mu_1500', 0), ('contents_LMD_mu_2500', 0), ('contents_LMJM_lambda_0.70', 0), ('contents_NTFIDF', 0), ('contents_Prob', 0), ('contents_Proximity', 0), ('contents_tpDistWindow100', 0), ('QueryLength', 0), ('contents_QueryCoverageRatio', 0), ('UniqueQueryTerms', 0), ('contents_MatchingTermCount', 0), ('contents_TF_avg', 0), ('contents_TF_median', 0), ('contents_TF_sum', 0), ('contents_TF_min', 0), ('contents_TF_max', 0), ('contents_TF_var', 0), ('contents_TF_maxminratio', 0), ('contents_TFIDF_min', 0), ('contents_TFIDF_var', 0), ('contents_TFIDF_maxminratio', 0), ('contents_TFIDF_confidence', 0), ('contents_SCQ_min', 0), ('contents_SCQ_max', 0), ('contents_NormalizedTF_avg', 0), ('contents_NormalizedTF_median', 0), ('contents_NormalizedTF_sum', 0), ('contents_NormalizedTF_min', 0), ('contents_NormalizedTF_max', 0), ('contents_IDF_avg', 0), ('contents_IDF_min', 0), ('contents_IDF_maxminratio', 0), ('contents_ICTF_sum', 0), ('contents_ICTF_min', 0), ('contents_ICTF_max', 0), ('contents_ICTF_confidence', 0), ('contents_UnorderedSequentialPairs_3', 0), ('contents_UnorderedSequentialPairs_8', 0), ('contents_UnorderedSequentialPairs_15', 0), ('contents_OrderedSequentialPairs_3', 0), ('contents_OrderedSequentialPairs_8', 0), ('contents_OrderedSequentialPairs_15', 0), ('contents_UnorderedQueryPairs_3', 0), ('contents_UnorderedQueryPairs_8', 0), ('contents_UnorderedQueryPairs_15', 0), ('contents_OrderedQueryPairs_8', 0), ('contents_OrderedQueryPairs_15', 0), ('Bert_IBMModel1', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [01:48<00:00, 64.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 6967618 times in 6980 queries\n",
      "recall@10:0.002793696275071633\n",
      "recall@20:0.006017191977077364\n",
      "recall@50:0.011580706781279846\n",
      "recall@100:0.019675262655205347\n",
      "recall@200:0.035936007640878696\n",
      "recall@500:0.08070678127984718\n",
      "recall@1000:0.8573424068767909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6980/6980 [00:45<00:00, 152.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_tie occurs 62745 times in 6979 queries 0.0006042206758539137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_extracted['data']['score'] = 0.\n",
    "for seed in [12345]:\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'lambdarank',\n",
    "            'max_bin':255,\n",
    "            'num_leaves':63,\n",
    "            'max_depth':-1,\n",
    "            'min_data_in_leaf':50,\n",
    "            'min_sum_hessian_in_leaf':0,\n",
    "#             'bagging_fraction':0.8,\n",
    "#             'bagging_freq':50,\n",
    "            'feature_fraction':1,\n",
    "            'learning_rate':0.1,\n",
    "            'num_boost_round':1000,\n",
    "            'early_stopping_round':200,\n",
    "            'metric':'custom',\n",
    "            'label_gain':[0,1],\n",
    "            'lambdarank_truncation_level':20,\n",
    "            'seed':seed,\n",
    "            'num_threads':max(multiprocessing.cpu_count()//2,1)\n",
    "    }\n",
    "    num_boost_round = params.pop('num_boost_round')\n",
    "    early_stopping_round = params.pop('early_stopping_round')\n",
    "    gbm = lgb.train(params, lgb_train,\n",
    "                    valid_sets=lgb_valid,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    early_stopping_rounds=early_stopping_round,\n",
    "                    feval=recall_at_200,\n",
    "                    feature_name=feature_name,\n",
    "                    verbose_eval=True)\n",
    "    dev_extracted['data']['score'] += gbm.predict(dev_X)\n",
    "    best_score = gbm.best_score['valid_0']['recall@200']\n",
    "    print(best_score)\n",
    "    best_iteration = gbm.best_iteration\n",
    "    print(best_iteration)\n",
    "    feature_importances = sorted(list(zip(feature_name,gbm.feature_importance().tolist())),\n",
    "                                 key=lambda x:x[1],reverse=True)\n",
    "    print(feature_importances)\n",
    "eval_recall(dev_qrel, dev_extracted['data'])\n",
    "eval_mrr(dev_extracted['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
